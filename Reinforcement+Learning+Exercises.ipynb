{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from the vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "Task: Create a reinforcement learning algorithm\n",
    "> - Using Q leaning\n",
    "> - Design interfaces to OpenAIGym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CartPole code: http://github.com/openai/mish/blob/master/gym/gym/envs/classic_control_cartpole.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym # pip install gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Episode finished after 19.000000 time steps / mean 0.000000\n",
      "1 Episode finished after 12.000000 time steps / mean -1.820000\n",
      "2 Episode finished after 17.000000 time steps / mean -3.710000\n",
      "3 Episode finished after 16.000000 time steps / mean -5.550000\n",
      "4 Episode finished after 15.000000 time steps / mean -7.400000\n",
      "5 Episode finished after 52.000000 time steps / mean -9.260000\n",
      "6 Episode finished after 47.000000 time steps / mean -10.750000\n",
      "7 Episode finished after 27.000000 time steps / mean -12.290000\n",
      "8 Episode finished after 24.000000 time steps / mean -14.030000\n",
      "9 Episode finished after 12.000000 time steps / mean -15.800000\n",
      "10 Episode finished after 34.000000 time steps / mean -17.690000\n",
      "11 Episode finished after 48.000000 time steps / mean -19.360000\n",
      "12 Episode finished after 28.000000 time steps / mean -20.890000\n",
      "13 Episode finished after 51.000000 time steps / mean -22.620000\n",
      "14 Episode finished after 13.000000 time steps / mean -24.120000\n",
      "15 Episode finished after 29.000000 time steps / mean -26.000000\n",
      "16 Episode finished after 32.000000 time steps / mean -27.720000\n",
      "17 Episode finished after 12.000000 time steps / mean -29.410000\n",
      "18 Episode finished after 19.000000 time steps / mean -31.300000\n",
      "19 Episode finished after 173.000000 time steps / mean -33.120000\n",
      "20 Episode finished after 54.000000 time steps / mean -33.400000\n",
      "21 Episode finished after 33.000000 time steps / mean -34.870000\n",
      "22 Episode finished after 39.000000 time steps / mean -36.550000\n",
      "23 Episode finished after 44.000000 time steps / mean -38.170000\n",
      "24 Episode finished after 33.000000 time steps / mean -39.740000\n",
      "25 Episode finished after 17.000000 time steps / mean -41.420000\n",
      "26 Episode finished after 148.000000 time steps / mean -43.260000\n",
      "27 Episode finished after 148.000000 time steps / mean -43.790000\n",
      "28 Episode finished after 29.000000 time steps / mean -44.320000\n",
      "29 Episode finished after 13.000000 time steps / mean -46.040000\n",
      "30 Episode finished after 14.000000 time steps / mean -47.920000\n",
      "31 Episode finished after 200.000000 time steps / mean -49.790000\n",
      "32 Episode finished after 17.000000 time steps / mean -47.790000\n",
      "33 Episode finished after 125.000000 time steps / mean -49.630000\n",
      "34 Episode finished after 134.000000 time steps / mean -50.390000\n",
      "35 Episode finished after 12.000000 time steps / mean -51.060000\n",
      "36 Episode finished after 56.000000 time steps / mean -52.950000\n",
      "37 Episode finished after 10.000000 time steps / mean -54.400000\n",
      "38 Episode finished after 10.000000 time steps / mean -56.310000\n",
      "39 Episode finished after 24.000000 time steps / mean -58.220000\n",
      "40 Episode finished after 30.000000 time steps / mean -59.990000\n",
      "41 Episode finished after 16.000000 time steps / mean -61.700000\n",
      "42 Episode finished after 14.000000 time steps / mean -63.550000\n",
      "43 Episode finished after 13.000000 time steps / mean -65.420000\n",
      "44 Episode finished after 12.000000 time steps / mean -67.300000\n",
      "45 Episode finished after 40.000000 time steps / mean -69.190000\n",
      "46 Episode finished after 13.000000 time steps / mean -70.800000\n",
      "47 Episode finished after 143.000000 time steps / mean -72.680000\n",
      "48 Episode finished after 110.000000 time steps / mean -73.260000\n",
      "49 Episode finished after 12.000000 time steps / mean -74.170000\n",
      "50 Episode finished after 9.000000 time steps / mean -76.060000\n",
      "51 Episode finished after 12.000000 time steps / mean -77.980000\n",
      "52 Episode finished after 125.000000 time steps / mean -79.870000\n",
      "53 Episode finished after 196.000000 time steps / mean -80.630000\n",
      "54 Episode finished after 151.000000 time steps / mean -78.670000\n",
      "55 Episode finished after 162.000000 time steps / mean -79.170000\n",
      "56 Episode finished after 54.000000 time steps / mean -79.560000\n",
      "57 Episode finished after 74.000000 time steps / mean -81.030000\n",
      "58 Episode finished after 148.000000 time steps / mean -82.300000\n",
      "59 Episode finished after 184.000000 time steps / mean -82.830000\n",
      "60 Episode finished after 175.000000 time steps / mean -83.000000\n",
      "61 Episode finished after 200.000000 time steps / mean -83.260000\n",
      "62 Episode finished after 200.000000 time steps / mean -81.260000\n",
      "63 Episode finished after 140.000000 time steps / mean -79.260000\n",
      "64 Episode finished after 171.000000 time steps / mean -79.870000\n",
      "65 Episode finished after 170.000000 time steps / mean -80.170000\n",
      "66 Episode finished after 10.000000 time steps / mean -80.480000\n",
      "67 Episode finished after 25.000000 time steps / mean -82.390000\n",
      "68 Episode finished after 145.000000 time steps / mean -84.150000\n",
      "69 Episode finished after 181.000000 time steps / mean -84.710000\n",
      "70 Episode finished after 147.000000 time steps / mean -84.910000\n",
      "71 Episode finished after 200.000000 time steps / mean -85.450000\n",
      "72 Episode finished after 189.000000 time steps / mean -83.450000\n",
      "73 Episode finished after 109.000000 time steps / mean -83.570000\n",
      "74 Episode finished after 157.000000 time steps / mean -84.490000\n",
      "75 Episode finished after 200.000000 time steps / mean -84.930000\n",
      "76 Episode finished after 157.000000 time steps / mean -82.930000\n",
      "77 Episode finished after 133.000000 time steps / mean -83.370000\n",
      "78 Episode finished after 117.000000 time steps / mean -84.050000\n",
      "79 Episode finished after 180.000000 time steps / mean -84.890000\n",
      "80 Episode finished after 200.000000 time steps / mean -85.100000\n",
      "81 Episode finished after 200.000000 time steps / mean -83.100000\n",
      "82 Episode finished after 200.000000 time steps / mean -81.100000\n",
      "83 Episode finished after 177.000000 time steps / mean -79.100000\n",
      "84 Episode finished after 181.000000 time steps / mean -79.340000\n",
      "85 Episode finished after 79.000000 time steps / mean -79.540000\n",
      "86 Episode finished after 182.000000 time steps / mean -80.760000\n",
      "87 Episode finished after 195.000000 time steps / mean -80.950000\n",
      "88 Episode finished after 165.000000 time steps / mean -81.010000\n",
      "89 Episode finished after 21.000000 time steps / mean -81.370000\n",
      "90 Episode finished after 187.000000 time steps / mean -83.170000\n",
      "91 Episode finished after 200.000000 time steps / mean -83.310000\n",
      "92 Episode finished after 168.000000 time steps / mean -81.310000\n",
      "93 Episode finished after 158.000000 time steps / mean -81.640000\n",
      "94 Episode finished after 200.000000 time steps / mean -82.070000\n",
      "95 Episode finished after 195.000000 time steps / mean -80.070000\n",
      "96 Episode finished after 147.000000 time steps / mean -80.130000\n",
      "97 Episode finished after 166.000000 time steps / mean -80.670000\n",
      "98 Episode finished after 157.000000 time steps / mean -81.020000\n",
      "99 Episode finished after 200.000000 time steps / mean -81.460000\n",
      "100 Episode finished after 172.000000 time steps / mean -79.460000\n",
      "101 Episode finished after 185.000000 time steps / mean -77.930000\n",
      "102 Episode finished after 200.000000 time steps / mean -76.200000\n",
      "103 Episode finished after 130.000000 time steps / mean -72.360000\n",
      "104 Episode finished after 171.000000 time steps / mean -71.220000\n",
      "105 Episode finished after 197.000000 time steps / mean -69.660000\n",
      "106 Episode finished after 105.000000 time steps / mean -66.200000\n",
      "107 Episode finished after 200.000000 time steps / mean -65.620000\n",
      "108 Episode finished after 200.000000 time steps / mean -61.880000\n",
      "109 Episode finished after 93.000000 time steps / mean -58.110000\n",
      "110 Episode finished after 13.000000 time steps / mean -57.300000\n",
      "111 Episode finished after 149.000000 time steps / mean -57.510000\n",
      "112 Episode finished after 89.000000 time steps / mean -56.500000\n",
      "113 Episode finished after 168.000000 time steps / mean -55.890000\n",
      "114 Episode finished after 189.000000 time steps / mean -54.720000\n",
      "115 Episode finished after 157.000000 time steps / mean -52.960000\n",
      "116 Episode finished after 170.000000 time steps / mean -51.680000\n",
      "117 Episode finished after 152.000000 time steps / mean -50.300000\n",
      "118 Episode finished after 200.000000 time steps / mean -48.900000\n",
      "119 Episode finished after 171.000000 time steps / mean -45.080000\n",
      "120 Episode finished after 157.000000 time steps / mean -45.100000\n",
      "121 Episode finished after 93.000000 time steps / mean -44.070000\n",
      "122 Episode finished after 200.000000 time steps / mean -43.470000\n",
      "123 Episode finished after 200.000000 time steps / mean -39.850000\n",
      "124 Episode finished after 117.000000 time steps / mean -36.280000\n",
      "125 Episode finished after 156.000000 time steps / mean -35.440000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 Episode finished after 200.000000 time steps / mean -34.050000\n",
      "127 Episode finished after 200.000000 time steps / mean -31.520000\n",
      "128 Episode finished after 200.000000 time steps / mean -28.990000\n",
      "129 Episode finished after 133.000000 time steps / mean -25.270000\n",
      "130 Episode finished after 200.000000 time steps / mean -24.070000\n",
      "131 Episode finished after 200.000000 time steps / mean -20.200000\n",
      "132 Episode finished after 177.000000 time steps / mean -20.200000\n",
      "133 Episode finished after 200.000000 time steps / mean -18.600000\n",
      "134 Episode finished after 175.000000 time steps / mean -15.840000\n",
      "135 Episode finished after 126.000000 time steps / mean -15.430000\n",
      "136 Episode finished after 172.000000 time steps / mean -14.290000\n",
      "137 Episode finished after 200.000000 time steps / mean -13.130000\n",
      "138 Episode finished after 138.000000 time steps / mean -9.220000\n",
      "139 Episode finished after 91.000000 time steps / mean -7.940000\n",
      "140 Episode finished after 154.000000 time steps / mean -7.270000\n",
      "141 Episode finished after 200.000000 time steps / mean -6.030000\n",
      "142 Episode finished after 200.000000 time steps / mean -2.180000\n",
      "143 Episode finished after 136.000000 time steps / mean 1.690000\n",
      "144 Episode finished after 200.000000 time steps / mean 2.920000\n",
      "145 Episode finished after 200.000000 time steps / mean 6.810000\n",
      "146 Episode finished after 148.000000 time steps / mean 10.420000\n",
      "147 Episode finished after 200.000000 time steps / mean 11.770000\n",
      "148 Episode finished after 151.000000 time steps / mean 14.350000\n",
      "149 Episode finished after 200.000000 time steps / mean 14.760000\n",
      "150 Episode finished after 200.000000 time steps / mean 18.650000\n",
      "151 Episode finished after 200.000000 time steps / mean 22.570000\n",
      "152 Episode finished after 200.000000 time steps / mean 26.460000\n",
      "153 Episode finished after 200.000000 time steps / mean 29.220000\n",
      "154 Episode finished after 195.000000 time steps / mean 29.260000\n",
      "155 Episode finished after 200.000000 time steps / mean 29.700000\n",
      "156 Episode finished after 200.000000 time steps / mean 32.090000\n",
      "157 Episode finished after 200.000000 time steps / mean 35.560000\n",
      "158 Episode finished after 200.000000 time steps / mean 38.830000\n",
      "159 Episode finished after 98.000000 time steps / mean 41.360000\n",
      "160 Episode finished after 200.000000 time steps / mean 40.500000\n",
      "161 Episode finished after 200.000000 time steps / mean 42.760000\n",
      "162 Episode finished after 200.000000 time steps / mean 42.760000\n",
      "163 Episode finished after 200.000000 time steps / mean 42.760000\n",
      "164 Episode finished after 141.000000 time steps / mean 45.370000\n",
      "165 Episode finished after 200.000000 time steps / mean 45.070000\n",
      "166 Episode finished after 200.000000 time steps / mean 47.380000\n",
      "167 Episode finished after 200.000000 time steps / mean 51.290000\n",
      "168 Episode finished after 200.000000 time steps / mean 55.050000\n",
      "169 Episode finished after 200.000000 time steps / mean 57.610000\n",
      "170 Episode finished after 200.000000 time steps / mean 59.810000\n",
      "171 Episode finished after 200.000000 time steps / mean 62.350000\n",
      "172 Episode finished after 119.000000 time steps / mean 62.350000\n",
      "173 Episode finished after 11.000000 time steps / mean 61.650000\n",
      "174 Episode finished after 200.000000 time steps / mean 60.670000\n",
      "175 Episode finished after 193.000000 time steps / mean 63.110000\n",
      "176 Episode finished after 161.000000 time steps / mean 61.030000\n",
      "177 Episode finished after 131.000000 time steps / mean 61.070000\n",
      "178 Episode finished after 191.000000 time steps / mean 61.050000\n",
      "179 Episode finished after 170.000000 time steps / mean 61.790000\n",
      "180 Episode finished after 200.000000 time steps / mean 61.690000\n",
      "181 Episode finished after 157.000000 time steps / mean 61.690000\n",
      "182 Episode finished after 143.000000 time steps / mean 59.250000\n",
      "183 Episode finished after 200.000000 time steps / mean 56.670000\n",
      "184 Episode finished after 179.000000 time steps / mean 58.910000\n",
      "185 Episode finished after 200.000000 time steps / mean 58.890000\n",
      "186 Episode finished after 200.000000 time steps / mean 62.110000\n",
      "187 Episode finished after 77.000000 time steps / mean 64.300000\n",
      "188 Episode finished after 200.000000 time steps / mean 63.120000\n",
      "189 Episode finished after 183.000000 time steps / mean 65.480000\n",
      "190 Episode finished after 155.000000 time steps / mean 67.100000\n",
      "191 Episode finished after 184.000000 time steps / mean 66.780000\n",
      "192 Episode finished after 200.000000 time steps / mean 64.610000\n",
      "193 Episode finished after 163.000000 time steps / mean 66.940000\n",
      "194 Episode finished after 200.000000 time steps / mean 66.990000\n",
      "195 Episode finished after 137.000000 time steps / mean 66.990000\n",
      "196 Episode finished after 136.000000 time steps / mean 66.410000\n",
      "197 Episode finished after 200.000000 time steps / mean 66.300000\n",
      "198 Episode finished after 164.000000 time steps / mean 68.650000\n",
      "199 Episode finished after 200.000000 time steps / mean 68.720000\n",
      "200 Episode finished after 143.000000 time steps / mean 68.720000\n",
      "201 Episode finished after 200.000000 time steps / mean 68.430000\n",
      "202 Episode finished after 148.000000 time steps / mean 70.590000\n",
      "203 Episode finished after 154.000000 time steps / mean 68.060000\n",
      "204 Episode finished after 155.000000 time steps / mean 68.300000\n",
      "205 Episode finished after 181.000000 time steps / mean 68.140000\n",
      "206 Episode finished after 200.000000 time steps / mean 65.970000\n",
      "207 Episode finished after 180.000000 time steps / mean 68.930000\n",
      "208 Episode finished after 194.000000 time steps / mean 66.720000\n",
      "209 Episode finished after 160.000000 time steps / mean 64.650000\n",
      "210 Episode finished after 195.000000 time steps / mean 65.320000\n",
      "211 Episode finished after 174.000000 time steps / mean 67.140000\n",
      "212 Episode finished after 196.000000 time steps / mean 67.390000\n",
      "213 Episode finished after 115.000000 time steps / mean 70.470000\n",
      "214 Episode finished after 173.000000 time steps / mean 69.940000\n",
      "215 Episode finished after 200.000000 time steps / mean 69.780000\n",
      "216 Episode finished after 164.000000 time steps / mean 72.220000\n",
      "217 Episode finished after 131.000000 time steps / mean 72.160000\n",
      "218 Episode finished after 200.000000 time steps / mean 71.950000\n",
      "219 Episode finished after 200.000000 time steps / mean 71.950000\n",
      "220 Episode finished after 200.000000 time steps / mean 74.250000\n",
      "221 Episode finished after 183.000000 time steps / mean 76.690000\n",
      "222 Episode finished after 148.000000 time steps / mean 77.590000\n",
      "223 Episode finished after 175.000000 time steps / mean 75.060000\n",
      "224 Episode finished after 200.000000 time steps / mean 72.800000\n",
      "225 Episode finished after 143.000000 time steps / mean 75.640000\n",
      "226 Episode finished after 132.000000 time steps / mean 75.510000\n",
      "227 Episode finished after 148.000000 time steps / mean 72.820000\n",
      "228 Episode finished after 200.000000 time steps / mean 70.290000\n",
      "229 Episode finished after 200.000000 time steps / mean 70.290000\n",
      "230 Episode finished after 189.000000 time steps / mean 72.970000\n",
      "231 Episode finished after 156.000000 time steps / mean 70.850000\n",
      "232 Episode finished after 130.000000 time steps / mean 68.400000\n",
      "233 Episode finished after 147.000000 time steps / mean 67.930000\n",
      "234 Episode finished after 170.000000 time steps / mean 65.390000\n",
      "235 Episode finished after 137.000000 time steps / mean 65.340000\n",
      "236 Episode finished after 141.000000 time steps / mean 65.450000\n",
      "237 Episode finished after 189.000000 time steps / mean 65.140000\n",
      "238 Episode finished after 172.000000 time steps / mean 63.020000\n",
      "239 Episode finished after 184.000000 time steps / mean 63.360000\n",
      "240 Episode finished after 48.000000 time steps / mean 64.290000\n",
      "241 Episode finished after 43.000000 time steps / mean 63.230000\n",
      "242 Episode finished after 182.000000 time steps / mean 59.650000\n",
      "243 Episode finished after 200.000000 time steps / mean 57.460000\n",
      "244 Episode finished after 156.000000 time steps / mean 60.110000\n",
      "245 Episode finished after 147.000000 time steps / mean 57.660000\n",
      "246 Episode finished after 180.000000 time steps / mean 55.120000\n",
      "247 Episode finished after 200.000000 time steps / mean 55.440000\n",
      "248 Episode finished after 169.000000 time steps / mean 55.440000\n",
      "249 Episode finished after 172.000000 time steps / mean 55.620000\n",
      "250 Episode finished after 172.000000 time steps / mean 53.330000\n",
      "251 Episode finished after 175.000000 time steps / mean 51.040000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252 Episode finished after 144.000000 time steps / mean 48.780000\n",
      "253 Episode finished after 106.000000 time steps / mean 46.210000\n",
      "254 Episode finished after 133.000000 time steps / mean 43.260000\n",
      "255 Episode finished after 144.000000 time steps / mean 42.640000\n",
      "256 Episode finished after 51.000000 time steps / mean 40.070000\n",
      "257 Episode finished after 127.000000 time steps / mean 36.570000\n",
      "258 Episode finished after 109.000000 time steps / mean 33.830000\n",
      "259 Episode finished after 143.000000 time steps / mean 30.910000\n",
      "260 Episode finished after 157.000000 time steps / mean 31.360000\n",
      "261 Episode finished after 11.000000 time steps / mean 28.920000\n",
      "262 Episode finished after 200.000000 time steps / mean 25.020000\n",
      "263 Episode finished after 200.000000 time steps / mean 25.020000\n",
      "264 Episode finished after 133.000000 time steps / mean 25.020000\n",
      "265 Episode finished after 200.000000 time steps / mean 24.940000\n",
      "266 Episode finished after 140.000000 time steps / mean 24.940000\n",
      "267 Episode finished after 102.000000 time steps / mean 22.330000\n",
      "268 Episode finished after 179.000000 time steps / mean 19.340000\n",
      "269 Episode finished after 200.000000 time steps / mean 17.120000\n",
      "270 Episode finished after 200.000000 time steps / mean 17.120000\n",
      "271 Episode finished after 200.000000 time steps / mean 17.120000\n",
      "272 Episode finished after 104.000000 time steps / mean 17.120000\n",
      "273 Episode finished after 200.000000 time steps / mean 16.970000\n",
      "274 Episode finished after 200.000000 time steps / mean 20.870000\n",
      "275 Episode finished after 200.000000 time steps / mean 20.870000\n",
      "276 Episode finished after 200.000000 time steps / mean 22.950000\n",
      "277 Episode finished after 19.000000 time steps / mean 25.350000\n",
      "278 Episode finished after 138.000000 time steps / mean 24.230000\n",
      "279 Episode finished after 165.000000 time steps / mean 23.700000\n",
      "280 Episode finished after 160.000000 time steps / mean 23.650000\n",
      "281 Episode finished after 176.000000 time steps / mean 21.240000\n",
      "282 Episode finished after 199.000000 time steps / mean 21.430000\n",
      "283 Episode finished after 153.000000 time steps / mean 24.000000\n",
      "284 Episode finished after 200.000000 time steps / mean 21.520000\n",
      "285 Episode finished after 194.000000 time steps / mean 23.740000\n",
      "286 Episode finished after 200.000000 time steps / mean 21.670000\n",
      "287 Episode finished after 200.000000 time steps / mean 21.670000\n",
      "288 Episode finished after 156.000000 time steps / mean 24.910000\n",
      "289 Episode finished after 142.000000 time steps / mean 22.460000\n",
      "290 Episode finished after 200.000000 time steps / mean 22.050000\n",
      "291 Episode finished after 179.000000 time steps / mean 24.510000\n",
      "292 Episode finished after 195.000000 time steps / mean 24.460000\n",
      "293 Episode finished after 200.000000 time steps / mean 22.400000\n",
      "294 Episode finished after 140.000000 time steps / mean 24.780000\n",
      "295 Episode finished after 178.000000 time steps / mean 22.170000\n",
      "296 Episode finished after 200.000000 time steps / mean 22.580000\n",
      "297 Episode finished after 142.000000 time steps / mean 25.230000\n",
      "298 Episode finished after 194.000000 time steps / mean 22.640000\n",
      "299 Episode finished after 153.000000 time steps / mean 22.940000\n",
      "300 Episode finished after 200.000000 time steps / mean 20.460000\n",
      "301 Episode finished after 103.000000 time steps / mean 23.040000\n",
      "302 Episode finished after 200.000000 time steps / mean 20.060000\n",
      "303 Episode finished after 121.000000 time steps / mean 22.590000\n",
      "304 Episode finished after 200.000000 time steps / mean 22.260000\n",
      "305 Episode finished after 200.000000 time steps / mean 24.720000\n",
      "306 Episode finished after 191.000000 time steps / mean 26.920000\n",
      "307 Episode finished after 187.000000 time steps / mean 24.820000\n",
      "308 Episode finished after 200.000000 time steps / mean 24.890000\n",
      "309 Episode finished after 200.000000 time steps / mean 26.960000\n",
      "310 Episode finished after 200.000000 time steps / mean 29.370000\n",
      "311 Episode finished after 101.000000 time steps / mean 31.430000\n",
      "312 Episode finished after 200.000000 time steps / mean 30.700000\n",
      "313 Episode finished after 130.000000 time steps / mean 30.740000\n",
      "314 Episode finished after 159.000000 time steps / mean 30.890000\n",
      "315 Episode finished after 103.000000 time steps / mean 30.750000\n",
      "316 Episode finished after 181.000000 time steps / mean 27.770000\n",
      "317 Episode finished after 200.000000 time steps / mean 27.940000\n",
      "318 Episode finished after 200.000000 time steps / mean 30.640000\n",
      "319 Episode finished after 200.000000 time steps / mean 30.640000\n",
      "320 Episode finished after 190.000000 time steps / mean 30.640000\n",
      "321 Episode finished after 183.000000 time steps / mean 28.530000\n",
      "322 Episode finished after 200.000000 time steps / mean 28.530000\n",
      "323 Episode finished after 192.000000 time steps / mean 31.060000\n",
      "324 Episode finished after 200.000000 time steps / mean 31.230000\n",
      "325 Episode finished after 174.000000 time steps / mean 31.230000\n",
      "326 Episode finished after 176.000000 time steps / mean 31.540000\n",
      "327 Episode finished after 200.000000 time steps / mean 31.980000\n",
      "328 Episode finished after 137.000000 time steps / mean 34.510000\n",
      "329 Episode finished after 200.000000 time steps / mean 31.870000\n",
      "330 Episode finished after 199.000000 time steps / mean 31.870000\n",
      "331 Episode finished after 166.000000 time steps / mean 33.980000\n",
      "332 Episode finished after 200.000000 time steps / mean 34.080000\n",
      "333 Episode finished after 196.000000 time steps / mean 36.790000\n",
      "334 Episode finished after 200.000000 time steps / mean 39.290000\n",
      "335 Episode finished after 173.000000 time steps / mean 41.600000\n",
      "336 Episode finished after 186.000000 time steps / mean 41.960000\n",
      "337 Episode finished after 27.000000 time steps / mean 42.410000\n",
      "338 Episode finished after 172.000000 time steps / mean 40.790000\n",
      "339 Episode finished after 200.000000 time steps / mean 40.790000\n",
      "340 Episode finished after 194.000000 time steps / mean 42.960000\n",
      "341 Episode finished after 200.000000 time steps / mean 44.420000\n",
      "342 Episode finished after 182.000000 time steps / mean 48.000000\n",
      "343 Episode finished after 200.000000 time steps / mean 48.000000\n",
      "344 Episode finished after 200.000000 time steps / mean 48.000000\n",
      "345 Episode finished after 200.000000 time steps / mean 50.450000\n",
      "346 Episode finished after 200.000000 time steps / mean 52.990000\n",
      "347 Episode finished after 200.000000 time steps / mean 55.200000\n",
      "348 Episode finished after 164.000000 time steps / mean 55.200000\n",
      "349 Episode finished after 200.000000 time steps / mean 55.150000\n",
      "350 Episode finished after 200.000000 time steps / mean 57.440000\n",
      "351 Episode finished after 200.000000 time steps / mean 59.730000\n",
      "352 Episode finished after 200.000000 time steps / mean 61.990000\n",
      "353 Episode finished after 200.000000 time steps / mean 64.560000\n",
      "354 Episode finished after 200.000000 time steps / mean 67.510000\n",
      "355 Episode finished after 200.000000 time steps / mean 70.190000\n",
      "356 Episode finished after 200.000000 time steps / mean 72.760000\n",
      "357 Episode finished after 168.000000 time steps / mean 76.260000\n",
      "358 Episode finished after 182.000000 time steps / mean 76.670000\n",
      "359 Episode finished after 200.000000 time steps / mean 77.400000\n",
      "360 Episode finished after 190.000000 time steps / mean 79.980000\n",
      "361 Episode finished after 200.000000 time steps / mean 80.310000\n",
      "362 Episode finished after 200.000000 time steps / mean 84.210000\n",
      "363 Episode finished after 200.000000 time steps / mean 84.210000\n",
      "364 Episode finished after 197.000000 time steps / mean 84.210000\n",
      "365 Episode finished after 142.000000 time steps / mean 86.860000\n",
      "366 Episode finished after 200.000000 time steps / mean 84.270000\n",
      "367 Episode finished after 163.000000 time steps / mean 86.880000\n",
      "368 Episode finished after 200.000000 time steps / mean 87.490000\n",
      "369 Episode finished after 170.000000 time steps / mean 89.710000\n",
      "370 Episode finished after 177.000000 time steps / mean 87.400000\n",
      "371 Episode finished after 200.000000 time steps / mean 85.160000\n",
      "372 Episode finished after 185.000000 time steps / mean 85.160000\n",
      "373 Episode finished after 168.000000 time steps / mean 85.970000\n",
      "374 Episode finished after 200.000000 time steps / mean 83.640000\n",
      "375 Episode finished after 189.000000 time steps / mean 83.640000\n",
      "376 Episode finished after 178.000000 time steps / mean 81.520000\n",
      "377 Episode finished after 174.000000 time steps / mean 79.290000\n",
      "378 Episode finished after 200.000000 time steps / mean 80.840000\n",
      "379 Episode finished after 136.000000 time steps / mean 83.470000\n",
      "380 Episode finished after 168.000000 time steps / mean 83.180000\n",
      "381 Episode finished after 200.000000 time steps / mean 83.260000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382 Episode finished after 194.000000 time steps / mean 85.510000\n",
      "383 Episode finished after 137.000000 time steps / mean 83.450000\n",
      "384 Episode finished after 200.000000 time steps / mean 83.290000\n",
      "385 Episode finished after 149.000000 time steps / mean 83.290000\n",
      "386 Episode finished after 200.000000 time steps / mean 82.840000\n",
      "387 Episode finished after 187.000000 time steps / mean 82.840000\n",
      "388 Episode finished after 166.000000 time steps / mean 80.700000\n",
      "389 Episode finished after 158.000000 time steps / mean 80.800000\n",
      "390 Episode finished after 200.000000 time steps / mean 80.960000\n",
      "391 Episode finished after 200.000000 time steps / mean 80.960000\n",
      "392 Episode finished after 153.000000 time steps / mean 83.180000\n",
      "393 Episode finished after 200.000000 time steps / mean 82.760000\n",
      "394 Episode finished after 137.000000 time steps / mean 82.760000\n",
      "395 Episode finished after 200.000000 time steps / mean 82.730000\n",
      "396 Episode finished after 162.000000 time steps / mean 84.960000\n",
      "397 Episode finished after 160.000000 time steps / mean 82.570000\n",
      "398 Episode finished after 200.000000 time steps / mean 82.750000\n",
      "399 Episode finished after 172.000000 time steps / mean 84.820000\n",
      "400 Episode finished after 200.000000 time steps / mean 85.010000\n",
      "401 Episode finished after 200.000000 time steps / mean 85.010000\n",
      "402 Episode finished after 200.000000 time steps / mean 87.990000\n",
      "403 Episode finished after 200.000000 time steps / mean 87.990000\n",
      "404 Episode finished after 200.000000 time steps / mean 90.790000\n",
      "405 Episode finished after 198.000000 time steps / mean 90.790000\n",
      "406 Episode finished after 200.000000 time steps / mean 90.770000\n",
      "407 Episode finished after 200.000000 time steps / mean 92.870000\n",
      "408 Episode finished after 61.000000 time steps / mean 95.010000\n",
      "409 Episode finished after 200.000000 time steps / mean 91.610000\n",
      "410 Episode finished after 200.000000 time steps / mean 91.610000\n",
      "411 Episode finished after 106.000000 time steps / mean 91.610000\n",
      "412 Episode finished after 200.000000 time steps / mean 91.660000\n",
      "413 Episode finished after 200.000000 time steps / mean 91.660000\n",
      "414 Episode finished after 200.000000 time steps / mean 94.370000\n",
      "415 Episode finished after 200.000000 time steps / mean 96.790000\n",
      "416 Episode finished after 187.000000 time steps / mean 99.770000\n",
      "417 Episode finished after 200.000000 time steps / mean 99.830000\n",
      "418 Episode finished after 178.000000 time steps / mean 99.830000\n",
      "419 Episode finished after 200.000000 time steps / mean 97.600000\n",
      "420 Episode finished after 200.000000 time steps / mean 97.600000\n",
      "421 Episode finished after 160.000000 time steps / mean 99.710000\n",
      "422 Episode finished after 200.000000 time steps / mean 99.480000\n",
      "423 Episode finished after 200.000000 time steps / mean 99.480000\n",
      "424 Episode finished after 159.000000 time steps / mean 101.570000\n",
      "425 Episode finished after 200.000000 time steps / mean 99.150000\n",
      "426 Episode finished after 170.000000 time steps / mean 101.420000\n",
      "427 Episode finished after 200.000000 time steps / mean 101.360000\n",
      "428 Episode finished after 191.000000 time steps / mean 101.360000\n",
      "429 Episode finished after 159.000000 time steps / mean 101.900000\n",
      "430 Episode finished after 152.000000 time steps / mean 99.480000\n",
      "431 Episode finished after 156.000000 time steps / mean 97.000000\n",
      "432 Episode finished after 182.000000 time steps / mean 96.900000\n",
      "433 Episode finished after 195.000000 time steps / mean 94.710000\n",
      "434 Episode finished after 200.000000 time steps / mean 92.690000\n",
      "435 Episode finished after 196.000000 time steps / mean 92.690000\n",
      "436 Episode finished after 200.000000 time steps / mean 94.930000\n",
      "437 Episode finished after 178.000000 time steps / mean 97.080000\n",
      "438 Episode finished after 162.000000 time steps / mean 98.590000\n",
      "439 Episode finished after 153.000000 time steps / mean 98.490000\n",
      "440 Episode finished after 200.000000 time steps / mean 96.010000\n",
      "441 Episode finished after 187.000000 time steps / mean 98.080000\n",
      "442 Episode finished after 142.000000 time steps / mean 95.940000\n",
      "443 Episode finished after 162.000000 time steps / mean 95.540000\n",
      "444 Episode finished after 153.000000 time steps / mean 93.150000\n",
      "445 Episode finished after 88.000000 time steps / mean 90.670000\n",
      "446 Episode finished after 31.000000 time steps / mean 87.540000\n",
      "447 Episode finished after 69.000000 time steps / mean 83.840000\n",
      "448 Episode finished after 200.000000 time steps / mean 80.520000\n",
      "449 Episode finished after 134.000000 time steps / mean 82.890000\n",
      "450 Episode finished after 200.000000 time steps / mean 80.220000\n",
      "451 Episode finished after 152.000000 time steps / mean 80.220000\n",
      "452 Episode finished after 200.000000 time steps / mean 77.730000\n",
      "453 Episode finished after 200.000000 time steps / mean 77.730000\n",
      "454 Episode finished after 156.000000 time steps / mean 77.730000\n",
      "455 Episode finished after 193.000000 time steps / mean 75.280000\n",
      "456 Episode finished after 200.000000 time steps / mean 73.200000\n",
      "457 Episode finished after 200.000000 time steps / mean 73.200000\n",
      "458 Episode finished after 196.000000 time steps / mean 75.530000\n",
      "459 Episode finished after 200.000000 time steps / mean 77.680000\n",
      "460 Episode finished after 200.000000 time steps / mean 77.680000\n",
      "461 Episode finished after 130.000000 time steps / mean 79.790000\n",
      "462 Episode finished after 183.000000 time steps / mean 77.080000\n",
      "463 Episode finished after 200.000000 time steps / mean 74.900000\n",
      "464 Episode finished after 200.000000 time steps / mean 74.900000\n",
      "465 Episode finished after 171.000000 time steps / mean 74.930000\n",
      "466 Episode finished after 145.000000 time steps / mean 75.220000\n",
      "467 Episode finished after 183.000000 time steps / mean 72.660000\n",
      "468 Episode finished after 150.000000 time steps / mean 72.860000\n",
      "469 Episode finished after 184.000000 time steps / mean 70.350000\n",
      "470 Episode finished after 80.000000 time steps / mean 70.490000\n",
      "471 Episode finished after 169.000000 time steps / mean 69.520000\n",
      "472 Episode finished after 105.000000 time steps / mean 67.200000\n",
      "473 Episode finished after 131.000000 time steps / mean 66.400000\n",
      "474 Episode finished after 102.000000 time steps / mean 66.030000\n",
      "475 Episode finished after 64.000000 time steps / mean 63.040000\n",
      "476 Episode finished after 100.000000 time steps / mean 61.790000\n",
      "477 Episode finished after 53.000000 time steps / mean 61.010000\n",
      "478 Episode finished after 48.000000 time steps / mean 59.800000\n",
      "479 Episode finished after 132.000000 time steps / mean 56.270000\n",
      "480 Episode finished after 194.000000 time steps / mean 56.230000\n",
      "481 Episode finished after 103.000000 time steps / mean 56.490000\n",
      "482 Episode finished after 137.000000 time steps / mean 53.510000\n",
      "483 Episode finished after 140.000000 time steps / mean 52.940000\n",
      "484 Episode finished after 200.000000 time steps / mean 52.970000\n",
      "485 Episode finished after 139.000000 time steps / mean 52.970000\n",
      "486 Episode finished after 65.000000 time steps / mean 52.870000\n",
      "487 Episode finished after 190.000000 time steps / mean 49.510000\n",
      "488 Episode finished after 200.000000 time steps / mean 49.540000\n",
      "489 Episode finished after 200.000000 time steps / mean 51.890000\n",
      "490 Episode finished after 145.000000 time steps / mean 54.320000\n",
      "491 Episode finished after 151.000000 time steps / mean 51.760000\n",
      "492 Episode finished after 165.000000 time steps / mean 49.260000\n",
      "493 Episode finished after 200.000000 time steps / mean 49.380000\n",
      "494 Episode finished after 172.000000 time steps / mean 49.380000\n",
      "495 Episode finished after 166.000000 time steps / mean 49.730000\n",
      "496 Episode finished after 200.000000 time steps / mean 47.380000\n",
      "497 Episode finished after 145.000000 time steps / mean 49.770000\n",
      "498 Episode finished after 136.000000 time steps / mean 49.620000\n",
      "499 Episode finished after 117.000000 time steps / mean 46.970000\n",
      "500 Episode finished after 121.000000 time steps / mean 46.420000\n",
      "501 Episode finished after 182.000000 time steps / mean 43.620000\n",
      "502 Episode finished after 189.000000 time steps / mean 41.430000\n",
      "503 Episode finished after 200.000000 time steps / mean 39.310000\n",
      "504 Episode finished after 200.000000 time steps / mean 39.310000\n",
      "505 Episode finished after 150.000000 time steps / mean 39.310000\n",
      "506 Episode finished after 196.000000 time steps / mean 36.820000\n",
      "507 Episode finished after 193.000000 time steps / mean 36.780000\n",
      "508 Episode finished after 200.000000 time steps / mean 34.700000\n",
      "509 Episode finished after 200.000000 time steps / mean 38.100000\n",
      "510 Episode finished after 200.000000 time steps / mean 38.100000\n",
      "511 Episode finished after 194.000000 time steps / mean 38.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 Episode finished after 200.000000 time steps / mean 38.980000\n",
      "513 Episode finished after 197.000000 time steps / mean 38.980000\n",
      "514 Episode finished after 189.000000 time steps / mean 38.950000\n",
      "515 Episode finished after 179.000000 time steps / mean 36.830000\n",
      "516 Episode finished after 174.000000 time steps / mean 34.610000\n",
      "517 Episode finished after 200.000000 time steps / mean 34.480000\n",
      "518 Episode finished after 164.000000 time steps / mean 34.480000\n",
      "519 Episode finished after 140.000000 time steps / mean 34.340000\n",
      "520 Episode finished after 163.000000 time steps / mean 31.730000\n",
      "521 Episode finished after 129.000000 time steps / mean 29.350000\n",
      "522 Episode finished after 136.000000 time steps / mean 29.040000\n",
      "523 Episode finished after 200.000000 time steps / mean 26.390000\n",
      "524 Episode finished after 200.000000 time steps / mean 26.390000\n",
      "525 Episode finished after 180.000000 time steps / mean 28.810000\n",
      "526 Episode finished after 200.000000 time steps / mean 26.600000\n",
      "527 Episode finished after 143.000000 time steps / mean 28.910000\n",
      "528 Episode finished after 200.000000 time steps / mean 26.330000\n",
      "529 Episode finished after 200.000000 time steps / mean 28.430000\n",
      "530 Episode finished after 137.000000 time steps / mean 30.850000\n",
      "531 Episode finished after 191.000000 time steps / mean 30.700000\n",
      "532 Episode finished after 139.000000 time steps / mean 31.050000\n",
      "533 Episode finished after 200.000000 time steps / mean 30.620000\n",
      "534 Episode finished after 128.000000 time steps / mean 32.680000\n",
      "535 Episode finished after 200.000000 time steps / mean 29.950000\n",
      "536 Episode finished after 190.000000 time steps / mean 29.990000\n",
      "537 Episode finished after 200.000000 time steps / mean 27.880000\n",
      "538 Episode finished after 200.000000 time steps / mean 30.110000\n",
      "539 Episode finished after 31.000000 time steps / mean 32.500000\n",
      "540 Episode finished after 42.000000 time steps / mean 31.280000\n",
      "541 Episode finished after 133.000000 time steps / mean 27.690000\n",
      "542 Episode finished after 90.000000 time steps / mean 27.150000\n",
      "543 Episode finished after 40.000000 time steps / mean 26.630000\n",
      "544 Episode finished after 119.000000 time steps / mean 25.410000\n",
      "545 Episode finished after 187.000000 time steps / mean 25.070000\n",
      "546 Episode finished after 200.000000 time steps / mean 26.060000\n",
      "547 Episode finished after 139.000000 time steps / mean 29.760000\n",
      "548 Episode finished after 115.000000 time steps / mean 30.460000\n",
      "549 Episode finished after 76.000000 time steps / mean 27.600000\n",
      "550 Episode finished after 169.000000 time steps / mean 27.020000\n",
      "551 Episode finished after 172.000000 time steps / mean 24.700000\n",
      "552 Episode finished after 156.000000 time steps / mean 24.900000\n",
      "553 Episode finished after 152.000000 time steps / mean 22.450000\n",
      "554 Episode finished after 162.000000 time steps / mean 19.960000\n",
      "555 Episode finished after 173.000000 time steps / mean 20.020000\n",
      "556 Episode finished after 200.000000 time steps / mean 19.820000\n",
      "557 Episode finished after 200.000000 time steps / mean 19.820000\n",
      "558 Episode finished after 200.000000 time steps / mean 19.820000\n",
      "559 Episode finished after 200.000000 time steps / mean 19.860000\n",
      "560 Episode finished after 200.000000 time steps / mean 19.860000\n",
      "561 Episode finished after 200.000000 time steps / mean 19.860000\n",
      "562 Episode finished after 199.000000 time steps / mean 22.570000\n",
      "563 Episode finished after 200.000000 time steps / mean 24.740000\n",
      "564 Episode finished after 200.000000 time steps / mean 24.740000\n",
      "565 Episode finished after 200.000000 time steps / mean 24.740000\n",
      "566 Episode finished after 127.000000 time steps / mean 27.040000\n",
      "567 Episode finished after 200.000000 time steps / mean 26.860000\n",
      "568 Episode finished after 199.000000 time steps / mean 29.040000\n",
      "569 Episode finished after 146.000000 time steps / mean 31.540000\n",
      "570 Episode finished after 102.000000 time steps / mean 31.160000\n",
      "571 Episode finished after 200.000000 time steps / mean 31.380000\n",
      "572 Episode finished after 138.000000 time steps / mean 33.700000\n",
      "573 Episode finished after 200.000000 time steps / mean 34.030000\n",
      "574 Episode finished after 200.000000 time steps / mean 36.730000\n",
      "575 Episode finished after 174.000000 time steps / mean 39.720000\n",
      "576 Episode finished after 173.000000 time steps / mean 40.820000\n",
      "577 Episode finished after 200.000000 time steps / mean 41.550000\n",
      "578 Episode finished after 189.000000 time steps / mean 45.030000\n",
      "579 Episode finished after 189.000000 time steps / mean 46.440000\n",
      "580 Episode finished after 200.000000 time steps / mean 47.010000\n",
      "581 Episode finished after 113.000000 time steps / mean 49.080000\n",
      "582 Episode finished after 200.000000 time steps / mean 49.180000\n",
      "583 Episode finished after 200.000000 time steps / mean 51.820000\n",
      "584 Episode finished after 197.000000 time steps / mean 54.430000\n",
      "585 Episode finished after 200.000000 time steps / mean 54.400000\n",
      "586 Episode finished after 122.000000 time steps / mean 57.020000\n",
      "587 Episode finished after 200.000000 time steps / mean 57.590000\n",
      "588 Episode finished after 168.000000 time steps / mean 59.700000\n",
      "589 Episode finished after 142.000000 time steps / mean 57.370000\n",
      "590 Episode finished after 172.000000 time steps / mean 54.780000\n",
      "591 Episode finished after 151.000000 time steps / mean 55.050000\n",
      "592 Episode finished after 193.000000 time steps / mean 55.050000\n",
      "593 Episode finished after 200.000000 time steps / mean 55.330000\n",
      "594 Episode finished after 185.000000 time steps / mean 55.330000\n",
      "595 Episode finished after 143.000000 time steps / mean 55.460000\n",
      "596 Episode finished after 200.000000 time steps / mean 55.230000\n",
      "597 Episode finished after 200.000000 time steps / mean 55.230000\n",
      "598 Episode finished after 180.000000 time steps / mean 57.790000\n",
      "599 Episode finished after 164.000000 time steps / mean 58.230000\n",
      "600 Episode finished after 200.000000 time steps / mean 58.700000\n",
      "601 Episode finished after 200.000000 time steps / mean 61.500000\n",
      "602 Episode finished after 164.000000 time steps / mean 63.690000\n",
      "603 Episode finished after 200.000000 time steps / mean 63.440000\n",
      "604 Episode finished after 188.000000 time steps / mean 63.440000\n",
      "605 Episode finished after 156.000000 time steps / mean 61.310000\n",
      "606 Episode finished after 200.000000 time steps / mean 61.370000\n",
      "607 Episode finished after 200.000000 time steps / mean 61.410000\n",
      "608 Episode finished after 173.000000 time steps / mean 63.490000\n",
      "609 Episode finished after 200.000000 time steps / mean 61.210000\n",
      "610 Episode finished after 200.000000 time steps / mean 61.210000\n",
      "611 Episode finished after 151.000000 time steps / mean 61.210000\n",
      "612 Episode finished after 171.000000 time steps / mean 60.780000\n",
      "613 Episode finished after 193.000000 time steps / mean 58.480000\n",
      "614 Episode finished after 200.000000 time steps / mean 56.430000\n",
      "615 Episode finished after 200.000000 time steps / mean 58.550000\n",
      "616 Episode finished after 144.000000 time steps / mean 60.770000\n",
      "617 Episode finished after 189.000000 time steps / mean 60.470000\n",
      "618 Episode finished after 200.000000 time steps / mean 58.350000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9c6c9f0515c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdigitize_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[0mq_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_Qtable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-9c6c9f0515c3>\u001b[0m in \u001b[0;36mdigitize_state\u001b[1;34m(observation)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mcart_pos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcart_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpole_angle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpole_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     digitized = [\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdigitize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcart_pos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbins\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2.4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_dizitized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdigitize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcart_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbins\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_dizitized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdigitize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpole_angle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbins\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_dizitized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mdigitize\u001b[1;34m(x, bins, right)\u001b[0m\n\u001b[0;32m   4807\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mside\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mside\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4808\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4809\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mside\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mside\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msearchsorted\u001b[1;34m(a, v, side, sorter)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m     \"\"\"\n\u001b[1;32m-> 1246\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'searchsorted'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mside\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mside\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msorter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "#create discretization samples\n",
    "def bins(clip_min, clip_max, num):\n",
    "    return np.linspace(clip_min, clip_max, num + 1)[1:-1]\n",
    "\n",
    "\n",
    "#discretize the state\n",
    "def digitize_state(observation):\n",
    "    cart_pos, cart_v, pole_angle, pole_v = observation\n",
    "    digitized = [\n",
    "        np.digitize(cart_pos, bins=bins(-2.4, 2.4, num_dizitized)),\n",
    "        np.digitize(cart_v, bins=bins(-3.0, 3.0, num_dizitized)),\n",
    "        np.digitize(pole_angle, bins=bins(-0.5, 0.5, num_dizitized)),\n",
    "        np.digitize(pole_v, bins=bins(-2.0, 2.0, num_dizitized))\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    return sum([x * (num_dizitized ** i) for i, x in enumerate(digitized)]) #state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#choose action by using epsilon greedy policy\n",
    "def get_action(next_state, episode):\n",
    "\n",
    "    epsilon = 0.5 * (1 / (episode + 1))\n",
    "    if epsilon <= np.random.uniform(0, 1):\n",
    "        next_action = np.argmax(q_table[next_state])\n",
    "    else:\n",
    "        next_action = np.random.choice([0, 1])\n",
    "    return next_action\n",
    "\n",
    "#building Q table: Q(s,a)= Q(s,a)+alpha[r+gamma maxQ(s_,a_)-Q(s,a)]\n",
    "def update_Qtable(q_table, state, action, reward, next_state):\n",
    "    gamma = 0.99\n",
    "    alpha = 0.5\n",
    "    next_Max_Q = max(q_table[next_state][0], q_table[next_state][1])\n",
    "    q_table[state, action] = (1 - alpha) * q_table[state, action] + \\\n",
    "                             alpha * (reward + gamma * next_Max_Q)\n",
    "\n",
    "    return q_table\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "max_number_of_steps = 200\n",
    "num_consecutive_iterations = 100\n",
    "num_episodes = 2000\n",
    "goal_average_reward = 195\n",
    "\n",
    "num_dizitized = 6\n",
    "q_table = np.random.uniform(\n",
    "    low=-1, high=1, size=(num_dizitized ** 4, env.action_space.n))\n",
    "\n",
    "total_reward_vec = np.zeros(num_consecutive_iterations)\n",
    "final_x = np.zeros((num_episodes, 1))\n",
    "islearned = 0\n",
    "isrender = 0\n",
    "\n",
    "#training\n",
    "for episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    state = digitize_state(observation)\n",
    "    action = np.argmax(q_table[state])\n",
    "    episode_reward = 0\n",
    "\n",
    "    for t in range(max_number_of_steps):\n",
    "        if islearned == 1:\n",
    "            env.render()\n",
    "            time.sleep(0.1)\n",
    "            print(observation[0])\n",
    "\n",
    "\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "\n",
    "        if done:\n",
    "            if t < 195:\n",
    "                reward = -200\n",
    "            else:\n",
    "                reward = 1\n",
    "        else:\n",
    "            reward = 1       # define reward as the task description\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "\n",
    "        next_state = digitize_state(observation)\n",
    "        q_table = update_Qtable(q_table, state, action, reward, next_state)\n",
    "\n",
    "\n",
    "        action = get_action(next_state, episode)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "\n",
    "        if done:\n",
    "            print('%d Episode finished after %f time steps / mean %f' %\n",
    "                  (episode, t + 1, total_reward_vec.mean()))\n",
    "            total_reward_vec = np.hstack((total_reward_vec[1:],\n",
    "                                          episode_reward))\n",
    "            if islearned == 1:\n",
    "                final_x[episode, 0] = observation[0]\n",
    "            break\n",
    "\n",
    "    if (total_reward_vec.mean() >=\n",
    "            goal_average_reward):\n",
    "        print('Episode %d train agent successfuly!' % episode)\n",
    "        islearned = 1\n",
    "        if isrender == 0:\n",
    "            isrender = 1\n",
    "\n",
    "\n",
    "if islearned:\n",
    "    np.savetxt('final_x.csv', final_x, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 6 x 6 simulator, where a blue circle can move right, left, up and down is given. That blue circle targets to reach to the yellow circle. Along the way there are black holes and green square objects. Since, those black holes and green square objects are obstacles, the blue circle should learn to reach to the target by avoiding collision with those  obstacles.\n",
    "\n",
    "Task 1: Change the environment method in order to assign the reward function to the agent as follows:\n",
    "\n",
    "> - -20 when blue circle falls into the black hole\n",
    "> - -10 when blue circle falls into the green square\n",
    "> -  100 when blue circle reaches to the yellow circle\n",
    "> -  0 else\n",
    "\n",
    "Task 2: Provide a Q learning algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "if sys.version_info.major == 2:\n",
    "    import Tkinter as tk\n",
    "else:\n",
    "    import tkinter as tk\n",
    "\n",
    "\n",
    "UNIT = 40   # pixels\n",
    "Grid_H = 6  # grid height\n",
    "Grid_W = 6  # grid width\n",
    "\n",
    "\n",
    "class Grid_Environmnet(tk.Tk, object):\n",
    "    def __init__(self):\n",
    "        super(Grid_Environmnet, self).__init__()\n",
    "        self.action_space = ['u', 'd', 'l', 'r']\n",
    "        self.n_actions = len(self.action_space)\n",
    "        self.title('Grid Environmnet')\n",
    "        self.geometry('{0}x{1}'.format(Grid_H * UNIT, Grid_H * UNIT))\n",
    "        self._build_Grid_Environmnet()\n",
    "\n",
    "\n",
    "    def _build_Grid_Environmnet(self):\n",
    "        self.canvas = tk.Canvas(self, bg='white',\n",
    "                           height=Grid_H * UNIT,\n",
    "                           width=Grid_W * UNIT)\n",
    "\n",
    "        # create grids\n",
    "        for c in range(0, Grid_W * UNIT, UNIT):\n",
    "            x0, y0, x1, y1 = c, 0, c, Grid_H * UNIT\n",
    "            self.canvas.create_line(x0, y0, x1, y1)\n",
    "        for r in range(0, Grid_H * UNIT, UNIT):\n",
    "            x0, y0, x1, y1 = 0, r, Grid_H * UNIT, r\n",
    "            self.canvas.create_line(x0, y0, x1, y1)\n",
    "\n",
    "        # create origin\n",
    "        origin = np.array([20, 20])\n",
    "\n",
    "        # hell\n",
    "        hell1_center = origin + np.array([UNIT *4, UNIT*3])\n",
    "        self.hell1 = self.canvas.create_rectangle(\n",
    "            hell1_center[0] - 15, hell1_center[1] - 15,\n",
    "            hell1_center[0] + 15, hell1_center[1] + 15,\n",
    "            fill='green')\n",
    "        # hell\n",
    "        hell2_center = origin + np.array([UNIT*3, UNIT * 4])\n",
    "        self.hell2 = self.canvas.create_rectangle(\n",
    "            hell2_center[0] - 15, hell2_center[1] - 15,\n",
    "            hell2_center[0] + 15, hell2_center[1] + 15,\n",
    "            fill='green')\n",
    "        # black hole\n",
    "        hell3_center = origin + np.array([0, UNIT*2])\n",
    "        self.hell3 = self.canvas.create_rectangle(\n",
    "            hell3_center[0] - 15, hell3_center[1] - 15,\n",
    "            hell3_center[0] + 15, hell3_center[1] + 15,\n",
    "            fill='black')\n",
    "\n",
    "        hell4_center = origin + np.array([UNIT*3, UNIT])\n",
    "        self.hell4 = self.canvas.create_rectangle(\n",
    "            hell4_center[0] - 15, hell4_center[1] - 15,\n",
    "            hell4_center[0] + 15, hell4_center[1] + 15,\n",
    "            fill='black')\n",
    "\n",
    "        # create oval\n",
    "        oval_center = origin + UNIT * 4\n",
    "        self.oval = self.canvas.create_oval(\n",
    "            oval_center[0] - 15, oval_center[1] - 15,\n",
    "            oval_center[0] + 15, oval_center[1] + 15,\n",
    "            fill='yellow')\n",
    "\n",
    "\n",
    "        oval_center = origin + UNIT * 2\n",
    "        self.oval_blue = self.canvas.create_oval(\n",
    "            oval_center[0] - 15, oval_center[1] - 15,\n",
    "            oval_center[0] + 15, oval_center[1] + 15,\n",
    "            fill='blue')\n",
    "        # pack all\n",
    "        self.canvas.pack()\n",
    "\n",
    "    def reset(self):\n",
    "        self.update()\n",
    "        time.sleep(0.5)\n",
    "        self.canvas.delete(self.oval_blue)\n",
    "        origin = np.array([20, 20])\n",
    "        self.oval_blue = self.canvas.create_oval(\n",
    "            origin[0] - 15, origin[1] - 15,\n",
    "            origin[0] + 15, origin[1] + 15,\n",
    "            fill='blue')\n",
    "        # return observation\n",
    "        return self.canvas.coords(self.oval_blue)\n",
    "\n",
    "    def step(self, action):\n",
    "        s = self.canvas.coords(self.oval_blue)\n",
    "        base_action = np.array([0, 0])\n",
    "        if action == 0:   # up\n",
    "            if s[1] > UNIT:\n",
    "                base_action[1] -= UNIT\n",
    "        elif action == 1:   # down\n",
    "            if s[1] < (Grid_H - 1) * UNIT:\n",
    "                base_action[1] += UNIT\n",
    "        elif action == 2:   # right\n",
    "            if s[0] < (Grid_W - 1) * UNIT:\n",
    "                base_action[0] += UNIT\n",
    "        elif action == 3:   # left\n",
    "            if s[0] > UNIT:\n",
    "                base_action[0] -= UNIT\n",
    "\n",
    "        self.canvas.move(self.oval_blue, base_action[0], base_action[1])  # move agent\n",
    "\n",
    "        s_ = self.canvas.coords(self.oval_blue)  # next state\n",
    "\n",
    "        # reward function\n",
    "        if s_ == self.canvas.coords(self.oval):\n",
    "            reward = 100\n",
    "            done = True\n",
    "        elif s_ in [self.canvas.coords(self.hell1), self.canvas.coords(self.hell2)]:\n",
    "            reward = -10\n",
    "            done = True\n",
    "        elif s_ in [self.canvas.coords(self.hell3)]:\n",
    "            reward = -20\n",
    "            done = True\n",
    "        elif s_ in [self.canvas.coords(self.hell4)]:\n",
    "            reward = -20\n",
    "            done = True\n",
    "        else:\n",
    "            reward = 0\n",
    "            done = False\n",
    "\n",
    "        return s_, reward, done\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        time.sleep(0.1)\n",
    "        self.update()\n",
    "\n",
    "\n",
    "def update():\n",
    "    for t in range(1):\n",
    "        s = env.reset()\n",
    "        while True:\n",
    "            env.render()\n",
    "            a = 1\n",
    "            s, r, done = env.step(a)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = Grid_Environmnet()\n",
    "    env.after(100, update)\n",
    "    env.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q learning for the Grid Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MEC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "C:\\Users\\MEC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "C:\\Users\\MEC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "C:\\Users\\MEC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "C:\\Users\\MEC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "C:\\Users\\MEC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: \n",
      "The current behaviour of 'Series.argmax' is deprecated, use 'idxmax'\n",
      "instead.\n",
      "The behavior of 'argmax' will be corrected to return the positional\n",
      "maximum in the future. For now, use 'series.values.argmax' or\n",
      "'np.argmax(np.array(values))' to get the position of the maximum\n",
      "row.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "-0.1\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "-0.1\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "-0.1\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "-0.1\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "-0.199\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\MEC\\Anaconda3\\lib\\tkinter\\__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\MEC\\Anaconda3\\lib\\tkinter\\__init__.py\", line 749, in callit\n",
      "    func(*args)\n",
      "  File \"<ipython-input-2-d03f54f9a27a>\", line 69, in update\n",
      "    observation_, reward, done = env.step(action)\n",
      "  File \"<ipython-input-1-75065677ede2>\", line 95, in step\n",
      "    s = self.canvas.coords(self.oval_blue)\n",
      "  File \"C:\\Users\\MEC\\Anaconda3\\lib\\tkinter\\__init__.py\", line 2469, in coords\n",
      "    self.tk.call((self._w, 'coords') + args))]\n",
      "_tkinter.TclError: invalid command name \".!canvas\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from Environment import Grid_Environmnet\n",
    "\n",
    "\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def chooseAction(self, observation):\n",
    "        self.checkStateExist(observation)\n",
    "\n",
    "        # action selection\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            state_action = self.q_table.ix[observation, :]\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))     # some actions have same value\n",
    "            action = state_action.argmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.checkStateExist(s_)\n",
    "        q_predict = self.q_table.ix[s, a]\n",
    "        if s_ != 'terminal':\n",
    "            q_target = r + self.gamma * self.q_table.ix[s_, :].max()  # next state is not terminal\n",
    "        else:\n",
    "            q_target = r  # next state is terminal\n",
    "        self.q_table.ix[s, a] += self.lr * (q_target - q_predict)  # update\n",
    "        print(self.q_table.ix[s,a])\n",
    "        #print(s)\n",
    "\n",
    "    def checkStateExist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series(\n",
    "                    [0]*len(self.actions),\n",
    "                    index=self.q_table.columns,\n",
    "                    name=state,\n",
    "\n",
    "                )\n",
    "            )\n",
    "            #print(self.q_table)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def update():\n",
    "    for episode in range(1000):\n",
    "        # initial observation\n",
    "        observation = env.reset()\n",
    "\n",
    "        while True:\n",
    "            # fresh env#\n",
    "            env.render()\n",
    "\n",
    "            # RL choose action based on observation\n",
    "            action = RL.chooseAction(str(observation))\n",
    "\n",
    "            # RL take action and get next observation and reward\n",
    "            observation_, reward, done = env.step(action)\n",
    "\n",
    "            # RL learn from this transition\n",
    "            RL.learn(str(observation), action, reward, str(observation_))\n",
    "\n",
    "            # swap observation\n",
    "            observation = observation_\n",
    "\n",
    "            # break while loop when end of this episode\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    # end of game\n",
    "    print('Search finished')\n",
    "    env.destroy()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = Grid_Environmnet()\n",
    "    RL = QLearningTable(actions=list(range(env.n_actions)))\n",
    "\n",
    "    env.after(1000, update)\n",
    "    env.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environement EnvRob contains two robots, which have mission of approaching the black box. This is happen, whenever the black box appears to be in the area of robots.\n",
    "\n",
    "Task: Design and implement the learning algorithm, which will enable the robots to accomplish the mission stated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EnvRob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyglet\n",
    "D = 0\n",
    "\n",
    "\n",
    "pyglet.clock.set_fps_limit(10000)\n",
    "# pyglet: Load images, sound, music and video in almost any format\n",
    "\n",
    "class ArmEnv(object):\n",
    "    action_bound = [-1, 1]\n",
    "    action_dim = 2\n",
    "    state_dim = 7\n",
    "    dt = .1  # refresh rate\n",
    "    arm1l = 100\n",
    "    arm2l = 100\n",
    "    arm1lb = 100\n",
    "    arm2lb = 100\n",
    "    viewer = None\n",
    "    viewer_xy = (400, 400)\n",
    "    #viewer = (800, 800)\n",
    "\n",
    "    vers = (100, 100)\n",
    "    get_point = False\n",
    "    mouse_in = np.array([False])\n",
    "    point_l = 15\n",
    "    grab_counter = 0\n",
    "\n",
    "    def __init__(self, mode='easy'):\n",
    "        self.mode = mode\n",
    "        self.arm_info = np.zeros((2, 4))\n",
    "        self.arm_infob = np.zeros((2, 4))\n",
    "        self.arm_info[0, 0] = self.arm1l\n",
    "        self.arm_info[1, 0] = self.arm2l\n",
    "        self.arm_infob[0, 0] = self.arm1lb\n",
    "        self.arm_infob[1, 0] = self.arm2lb\n",
    "        self.point_info = np.array([250, 303])\n",
    "        self.point_info_init = self.point_info.copy()\n",
    "        self.center_coord = np.array(self.viewer_xy)/2\n",
    "\n",
    "\n",
    "    def step1(self, action):\n",
    "        #action = (node1 angular v, node2 angular v)\n",
    "        action = np.clip(action, *self.action_bound)\n",
    "        self.arm_info[:, 1] += action * self.dt\n",
    "        self.arm_info[:, 1] %= np.pi * 2\n",
    "\n",
    "        arm1rad = self.arm_info[0, 1]\n",
    "        arm2rad = self.arm_info[1, 1]\n",
    "        arm1dx_dy = np.array([self.arm_info[0, 0] * np.cos(arm1rad), self.arm_info[0, 0] * np.sin(arm1rad)])\n",
    "        arm2dx_dy = np.array([self.arm_info[1, 0] * np.cos(arm2rad), self.arm_info[1, 0] * np.sin(arm2rad)])\n",
    "        self.arm_info[0, 2:4] = self.center_coord + arm1dx_dy  # (x1, y1)\n",
    "        self.arm_info[1, 2:4] = self.arm_info[0, 2:4] + arm2dx_dy  # (x2, y2)\n",
    "\n",
    "        s, arm2_distance = self._get_state1()\n",
    "        r1= self._r_func1(arm2_distance, center_coord=self.center_coord)\n",
    "        return s, r1, self.get_point\n",
    "\n",
    "\n",
    "    def step2(self, action):\n",
    "        # action = (node1 angular v, node2 angular v)\n",
    "        action = np.clip(action, *self.action_bound)\n",
    "        self.arm_infob[:, 1] += action * self.dt\n",
    "        self.arm_infob[:, 1] %= np.pi * 2\n",
    "\n",
    "        arm1radb = self.arm_infob[0, 1]\n",
    "        arm2radb = self.arm_infob[1, 1]\n",
    "        arm1dx_dyb = np.array([self.arm_infob[0, 0] * np.cos(arm1radb), self.arm_infob[0, 0] * np.sin(arm1radb)])\n",
    "        arm2dx_dyb = np.array([self.arm_infob[1, 0] * np.cos(arm2radb), self.arm_infob[1, 0] * np.sin(arm2radb)])\n",
    "        self.arm_infob[0, 2:4] = self.center_coord + arm1dx_dyb  # (x1, y1)\n",
    "        self.arm_infob[1, 2:4] = self.arm_infob[0, 2:4] + arm2dx_dyb  # (x2, y2)\n",
    "\n",
    "\n",
    "        s, arm2b_distance = self._get_state2()\n",
    "        r2 = self._r_func2(arm2b_distance, center_coord = -self.center_coord)\n",
    "        return s, r2, self.get_point\n",
    "\n",
    "    def reset1(self):\n",
    "        self.get_point = False\n",
    "        self.grab_counter = 0\n",
    "\n",
    "        if self.mode == 'hard':\n",
    "            pxy = np.clip(np.random.rand(2) * self.viewer_xy[0], 150, 250)\n",
    "            self.point_info[:] = pxy\n",
    "        else:\n",
    "            arm1rad, arm2rad = np.random.rand(2) * np.pi * 2\n",
    "            self.arm_info[0, 1] = arm1rad\n",
    "            self.arm_info[1, 1] = arm2rad\n",
    "            arm1dx_dy = np.array([self.arm_info[0, 0] * np.cos(arm1rad), self.arm_info[0, 0] * np.sin(arm1rad)])\n",
    "            arm2dx_dy = np.array([self.arm_info[1, 0] * np.cos(arm2rad), self.arm_info[1, 0] * np.sin(arm2rad)])\n",
    "            self.arm_info[0, 2:4] = self.center_coord + arm1dx_dy  # (x1, y1)\n",
    "            self.arm_info[1, 2:4] = self.arm_info[0, 2:4] + arm2dx_dy  # (x2, y2)\n",
    "\n",
    "\n",
    "            self.point_info[:] = self.point_info_init\n",
    "        return self._get_state1()[0]\n",
    "\n",
    "    def reset2(self):\n",
    "        self.get_point = False\n",
    "        self.grab_counter = 0\n",
    "\n",
    "        if self.mode == 'hard':\n",
    "            pxy = np.clip(np.random.rand(2) * self.viewer_xy[0], 150, 250)\n",
    "            self.point_info[:] = pxy\n",
    "        else:\n",
    "            arm1rad, arm2rad = np.random.rand(2) * np.pi * 2\n",
    "            self.arm_info[0, 1] = arm1rad\n",
    "            self.arm_info[1, 1] = arm2rad\n",
    "            arm1dx_dy = np.array([self.arm_info[0, 0] * np.cos(arm1rad), self.arm_info[0, 0] * np.sin(arm1rad)])\n",
    "            arm2dx_dy = np.array([self.arm_info[1, 0] * np.cos(arm2rad), self.arm_info[1, 0] * np.sin(arm2rad)])\n",
    "            self.arm_info[0, 2:4] = self.center_coord + arm1dx_dy  # (x1, y1)\n",
    "            self.arm_info[1, 2:4] = self.arm_info[0, 2:4] + arm2dx_dy  # (x2, y2)\n",
    "\n",
    "\n",
    "            arm1radb, arm2radb = np.random.rand(2) * np.pi * 2\n",
    "            self.arm_infob[0, 1] = arm1radb\n",
    "            self.arm_infob[1, 1] = arm2radb\n",
    "            arm1dx_dyb = np.array([self.arm_infob[0, 0] * np.cos(arm1radb), self.arm_infob[0, 0] * np.sin(arm1radb)])\n",
    "            arm2dx_dyb = np.array([self.arm_infob[1, 0] * np.cos(arm2radb), self.arm_infob[1, 0] * np.sin(arm2radb)])\n",
    "            self.arm_infob[0, 2:4] = self.center_coord + arm1dx_dyb  # (x1, y1)\n",
    "            self.arm_infob[1, 2:4] = self.arm_infob[0, 2:4] + arm2dx_dyb  # (x2, y2)\n",
    "\n",
    "            self.point_info[:] = self.point_info_init\n",
    "        return self._get_state2()[0]\n",
    "\n",
    "    def render(self):\n",
    "        if self.viewer is None:\n",
    "            self.viewer = Viewer(*self.viewer_xy, self.arm_info, self.arm_infob, self.point_info, self.point_l, self.mouse_in)\n",
    "        self.viewer.render()\n",
    "\n",
    "\n",
    "\n",
    "    def sample_action(self):\n",
    "        return np.random.uniform(*self.action_bound, size=self.action_dim)\n",
    "\n",
    "    def set_fps(self, fps=30):\n",
    "        pyglet.clock.set_fps_limit(fps)\n",
    "\n",
    "    def _get_state1(self):\n",
    "        # return the distance (dx, dy) between arm finger point with blue point\n",
    "        arm_end = self.arm_info[:, 2:4]\n",
    "        t_arms = np.ravel(arm_end - self.point_info)\n",
    "        center_dis = (self.center_coord - self.point_info) / 200\n",
    "        in_point = 1 if self.grab_counter > 0 else 0\n",
    "        return np.hstack([in_point, t_arms / 200, center_dis,\n",
    "                          ]), t_arms[-2:]\n",
    "\n",
    "    def _get_state2(self):\n",
    "        # return the distance (dx, dy) between arm finger point with blue point\n",
    "        arm_end = self.arm_info[:, 2:4]\n",
    "        arm_endb = self.arm_infob[:, 2:4]\n",
    "        t_arms = np.ravel(arm_end - self.point_info)\n",
    "        t_armsb = np.ravel(arm_endb - self.point_info)\n",
    "        center_dis = (self.center_coord - self.point_info)/200\n",
    "        in_point = 1 if self.grab_counter > 0 else 0\n",
    "        return np.hstack([in_point, t_armsb/200, center_dis,\n",
    "                          ]), t_armsb[-2:]\n",
    "\n",
    "\n",
    "\n",
    "    def _r_func1(self, distance, center_coord):\n",
    "        t = 50\n",
    "        abs_distance = np.sqkrt(np.sum(np.square(distance+center_coord/2)))\n",
    "        r1 = -abs_distance/200\n",
    "        if abs_distance < self.point_l and (not self.get_point):\n",
    "            r1 += 1.\n",
    "            self.grab_counter += 1\n",
    "            if self.grab_counter > t:\n",
    "                r1 += 10.\n",
    "                self.get_point = True\n",
    "        elif abs_distance > self.point_l:\n",
    "            self.grab_counter = 0\n",
    "            self.get_point = False\n",
    "        return r1\n",
    "\n",
    "    def _r_func2(self, distance, center_coord):\n",
    "        t = 50\n",
    "        abs_distance = np.sqrt(np.sum(np.square(distance + center_coord / 2)))\n",
    "        r2 = -abs_distance / 200\n",
    "        if abs_distance < self.point_l and (not self.get_point):\n",
    "            r2 += 1.\n",
    "            self.grab_counter += 1\n",
    "            if self.grab_counter > t:\n",
    "                r2 += 10.\n",
    "                self.get_point = True\n",
    "        elif abs_distance > self.point_l:\n",
    "            self.grab_counter = 0\n",
    "            self.get_point = False\n",
    "        return r2\n",
    "\n",
    "\n",
    "class Viewer(pyglet.window.Window):\n",
    "    color = {\n",
    "        'background': [1]*3 + [1]\n",
    "    }\n",
    "    fps_display = pyglet.clock.ClockDisplay()\n",
    "    bar_thc = 5\n",
    "\n",
    "    def __init__(self, width, height, arm_info, arm_infob, point_info, point_l, mouse_in):\n",
    "        super(Viewer, self).__init__(width, height, resizable=False, caption='Arm', vsync=False)  # vsync=False to not use the monitor FPS\n",
    "        self.set_location(x=80, y=10)\n",
    "        pyglet.gl.glClearColor(*self.color['background'])\n",
    "\n",
    "        self.arm_info = arm_info\n",
    "        self.arm_infob = arm_infob\n",
    "        self.point_info = point_info\n",
    "        self.mouse_in = mouse_in\n",
    "        self.point_l = point_l\n",
    "\n",
    "        self.center_coord = np.array((min(width, height)/2, ) * 2)\n",
    "        self.batch = pyglet.graphics.Batch()\n",
    "\n",
    "        arm1_box, arm2_box, point_box, arm1b_box, arm2b_box = [0]*8, [0]*8, [0]*8, [0]*8, [0]*8\n",
    "        c1, c2, c3 = (255, 127, 36) * 4, (0, 0, 0) * 4, (0, 0, 0) * 4\n",
    "        self.point = self.batch.add(4, pyglet.gl.GL_QUADS, None, ('v2f', point_box), ('c3B', c2))\n",
    "        self.arm1 = self.batch.add(4, pyglet.gl.GL_QUADS, None, ('v2f', arm1_box), ('c3B', c1))\n",
    "        self.arm2 = self.batch.add(4, pyglet.gl.GL_QUADS, None, ('v2f', arm2_box), ('c3B', c1))\n",
    "        self.arm1b = self.batch.add(4, pyglet.gl.GL_QUADS, None, ('v2f', arm1b_box), ('c3B', c1))\n",
    "        self.arm2b = self.batch.add(4, pyglet.gl.GL_QUADS, None, ('v2f', arm2b_box), ('c3B', c1))\n",
    "\n",
    "    def render(self):\n",
    "        pyglet.clock.tick()\n",
    "        self._update_arm()\n",
    "        self.switch_to()\n",
    "        self.dispatch_events()\n",
    "        self.dispatch_event('on_draw')\n",
    "        self.flip()\n",
    "\n",
    "    def on_draw(self):\n",
    "        self.clear()\n",
    "        self.batch.draw()\n",
    "\n",
    "\n",
    "    def _update_arm(self):\n",
    "        point_l = self.point_l\n",
    "        #center_coord = self.center_coord\n",
    "        point_box = (self.point_info[0] - point_l, self.point_info[1] - point_l,\n",
    "                     self.point_info[0] + point_l, self.point_info[1] - point_l,\n",
    "                     self.point_info[0] + point_l, self.point_info[1] + point_l,\n",
    "                     self.point_info[0] - point_l, self.point_info[1] + point_l)\n",
    "        self.point.vertices = point_box\n",
    "\n",
    "        arm1_coord = (*self.center_coord + self.center_coord / 2, *(self.arm_info[0, 2:4]) + self.center_coord / 2)  # (x0, y0, x1, y1)\n",
    "        arm2_coord = (*(self.arm_info[0, 2:4]) + self.center_coord / 2,\n",
    "            *(self.arm_info[1, 2:4]) + self.center_coord / 2)  # (x1, y1, x2, y2)\n",
    "\n",
    "\n",
    "        arm1_thick_rad = np.pi / 2 - self.arm_info[0, 1]\n",
    "        x01, y01 = arm1_coord[0] - np.cos(arm1_thick_rad) * self.bar_thc, arm1_coord[1] + np.sin(\n",
    "            arm1_thick_rad) * self.bar_thc\n",
    "        x02, y02 = arm1_coord[0] + np.cos(arm1_thick_rad) * self.bar_thc, arm1_coord[1] - np.sin(\n",
    "            arm1_thick_rad) * self.bar_thc\n",
    "        x11, y11 = arm1_coord[2] + np.cos(arm1_thick_rad) * self.bar_thc, arm1_coord[3] - np.sin(\n",
    "            arm1_thick_rad) * self.bar_thc\n",
    "        x12, y12 = arm1_coord[2] - np.cos(arm1_thick_rad) * self.bar_thc, arm1_coord[3] + np.sin(\n",
    "            arm1_thick_rad) * self.bar_thc\n",
    "        arm1_box = (x01, y01, x02, y02, x11, y11, x12, y12)\n",
    "        arm2_thick_rad = np.pi / 2 - self.arm_info[1, 1]\n",
    "        x11_, y11_ = arm2_coord[0] + np.cos(arm2_thick_rad) * self.bar_thc, arm2_coord[1] - np.sin(\n",
    "            arm2_thick_rad) * self.bar_thc\n",
    "        x12_, y12_ = arm2_coord[0] - np.cos(arm2_thick_rad) * self.bar_thc, arm2_coord[1] + np.sin(\n",
    "            arm2_thick_rad) * self.bar_thc\n",
    "        x21, y21 = arm2_coord[2] - np.cos(arm2_thick_rad) * self.bar_thc, arm2_coord[3] + np.sin(\n",
    "            arm2_thick_rad) * self.bar_thc\n",
    "        x22, y22 = arm2_coord[2] + np.cos(arm2_thick_rad) * self.bar_thc, arm2_coord[3] - np.sin(\n",
    "            arm2_thick_rad) * self.bar_thc\n",
    "        arm2_box = (x11_, y11_, x12_, y12_, x21, y21, x22, y22)\n",
    "        self.arm1.vertices = arm1_box\n",
    "        self.arm2.vertices = arm2_box\n",
    "\n",
    "        arm1b_coord = (*self.center_coord - self.center_coord/2, *(self.arm_infob[0, 2:4]) - self.center_coord/2)  # (x0, y0, x1, y1)\n",
    "        arm2b_coord = (\n",
    "        *(self.arm_infob[0, 2:4]) - self.center_coord/2, *(self.arm_infob[1, 2:4]) - self.center_coord/2)  # (x1, y1, x2, y2)\n",
    "\n",
    "\n",
    "        arm1b_thick_rad = np.pi / 2 - self.arm_infob[0, 1]\n",
    "        x01b, y01b = arm1b_coord[0] - np.cos(arm1b_thick_rad) * self.bar_thc, arm1b_coord[1] + np.sin(\n",
    "            arm1b_thick_rad) * self.bar_thc\n",
    "        x02b, y02b = arm1b_coord[0] + np.cos(arm1b_thick_rad) * self.bar_thc, arm1b_coord[1] - np.sin(\n",
    "            arm1b_thick_rad) * self.bar_thc\n",
    "        x11b, y11b = arm1b_coord[2] + np.cos(arm1b_thick_rad) * self.bar_thc, arm1b_coord[3] - np.sin(\n",
    "            arm1b_thick_rad) * self.bar_thc\n",
    "        x12b, y12b = arm1b_coord[2] - np.cos(arm1b_thick_rad) * self.bar_thc, arm1b_coord[3] + np.sin(\n",
    "            arm1b_thick_rad) * self.bar_thc\n",
    "        arm1b_box = (x01b, y01b, x02b, y02b, x11b, y11b, x12b, y12b)\n",
    "        arm2b_thick_rad = np.pi / 2 - self.arm_infob[1, 1]\n",
    "        x11b_, y11b_ = arm2b_coord[0] + np.cos(arm2b_thick_rad) * self.bar_thc, arm2b_coord[1] - np.sin(\n",
    "            arm2b_thick_rad) * self.bar_thc\n",
    "        x12b_, y12b_ = arm2b_coord[0] - np.cos(arm2b_thick_rad) * self.bar_thc, arm2b_coord[1] + np.sin(\n",
    "            arm2b_thick_rad) * self.bar_thc\n",
    "        x21b, y21b = arm2b_coord[2] - np.cos(arm2b_thick_rad) * self.bar_thc, arm2b_coord[3] + np.sin(\n",
    "            arm2b_thick_rad) * self.bar_thc\n",
    "        x22b, y22b = arm2b_coord[2] + np.cos(arm2b_thick_rad) * self.bar_thc, arm2b_coord[3] - np.sin(\n",
    "            arm2b_thick_rad) * self.bar_thc\n",
    "        arm2b_box = (x11b_, y11b_, x12b_, y12b_, x21b, y21b, x22b, y22b)\n",
    "        self.arm1b.vertices = arm1b_box\n",
    "        self.arm2b.vertices = arm2b_box\n",
    "\n",
    "    def on_key_press(self, symbol, modifiers):\n",
    "        if symbol == pyglet.window.key.UP:\n",
    "            self.arm_info[0, 1] += .1\n",
    "            print(self.arm_info[:, 2:4] - self.point_info)\n",
    "            self.arm_infob[0, 1] += .1\n",
    "            print(self.arm_infob[:, 2:4] - self.point_info)\n",
    "        elif symbol == pyglet.window.key.DOWN:\n",
    "            self.arm_info[0, 1] -= .1\n",
    "            print(self.arm_info[:, 2:4] - self.point_info)\n",
    "            self.arm_infob[0, 1] -= .1\n",
    "            print(self.arm_infob[:, 2:4] - self.point_info)\n",
    "        elif symbol == pyglet.window.key.LEFT:\n",
    "            self.arm_info[1, 1] += .1\n",
    "            print(self.arm_info[:, 2:4] - self.point_info)\n",
    "            self.arm_infob[1, 1] += .1\n",
    "            print(self.arm_infob[:, 2:4] - self.point_info)\n",
    "            self.arm_infob[1, 1] += .1\n",
    "            print(self.arm_infob[:, 2:4] - self.point_info)\n",
    "            self.arm_infob[1, 1] += .1\n",
    "            print(self.arm_infob[:, 2:4] - self.point_info)\n",
    "        elif symbol == pyglet.window.key.RIGHT:\n",
    "            self.arm_info[1, 1] -= .1\n",
    "            print(self.arm_info[:, 2:4] - self.point_info)\n",
    "            self.arm_infob[1, 1] -= .1\n",
    "            print(self.arm_infob[:, 2:4] - self.point_info)\n",
    "        elif symbol == pyglet.window.key.Q:\n",
    "            pyglet.clock.set_fps_limit(1000)\n",
    "        elif symbol == pyglet.window.key.A:\n",
    "            pyglet.clock.set_fps_limit(30)\n",
    "\n",
    "    def on_mouse_motion(self, x, y, dx, dy):\n",
    "        self.point_info[:] = [x, y]\n",
    "\n",
    "    def on_mouse_enter(self, x, y):\n",
    "        self.mouse_in[0] = True\n",
    "\n",
    "    def on_mouse_leave(self, x, y):\n",
    "        self.mouse_in[0] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-3-a6f3e30dd8b0>:84: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From C:\\Users\\MEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Ep1: 0 | ---- | R1: -381 | Explore: 2.00\n",
      "Ep2: 0 | ---- | R2: -144 | Explore2: 2.00\n",
      "Ep1: 1 | ---- | R1: -360 | Explore: 2.00\n",
      "Ep2: 1 | ---- | R2: -23 | Explore2: 2.00\n",
      "Ep1: 2 | ---- | R1: -259 | Explore: 2.00\n",
      "Ep2: 2 | ---- | R2: -84 | Explore2: 2.00\n",
      "Ep1: 3 | ---- | R1: -204 | Explore: 2.00\n",
      "Ep2: 3 | ---- | R2: -89 | Explore2: 2.00\n",
      "Ep1: 4 | ---- | R1: -191 | Explore: 2.00\n",
      "Ep2: 4 | ---- | R2: -135 | Explore2: 2.00\n",
      "Ep1: 5 | ---- | R1: -135 | Explore: 2.00\n",
      "Ep2: 5 | ---- | R2: -128 | Explore2: 2.00\n",
      "Ep1: 6 | ---- | R1: -214 | Explore: 2.00\n",
      "Ep2: 6 | ---- | R2: -127 | Explore2: 2.00\n",
      "Ep1: 7 | ---- | R1: -71 | Explore: 2.00\n",
      "Ep2: 7 | ---- | R2: -219 | Explore2: 2.00\n",
      "Ep1: 8 | ---- | R1: -127 | Explore: 2.00\n",
      "Ep2: 8 | ---- | R2: -322 | Explore2: 2.00\n",
      "Ep1: 9 | ---- | R1: -229 | Explore: 2.00\n",
      "Ep2: 9 | ---- | R2: -238 | Explore2: 2.00\n",
      "Ep1: 10 | ---- | R1: -66 | Explore: 2.00\n",
      "Ep2: 10 | ---- | R2: -367 | Explore2: 2.00\n",
      "Ep1: 11 | ---- | R1: -95 | Explore: 2.00\n",
      "Ep2: 11 | ---- | R2: -345 | Explore2: 2.00\n",
      "Ep1: 12 | ---- | R1: -115 | Explore: 2.00\n",
      "Ep2: 12 | ---- | R2: -267 | Explore2: 2.00\n",
      "Ep1: 13 | ---- | R1: -37 | Explore: 2.00\n",
      "Ep2: 13 | ---- | R2: -166 | Explore2: 2.00\n",
      "Ep1: 14 | ---- | R1: -70 | Explore: 2.00\n",
      "Ep2: 14 | ---- | R2: -259 | Explore2: 2.00\n",
      "Ep1: 15 | ---- | R1: -149 | Explore: 2.00\n",
      "Ep2: 15 | ---- | R2: -251 | Explore2: 2.00\n",
      "Ep1: 16 | ---- | R1: -82 | Explore: 2.00\n",
      "Ep2: 16 | ---- | R2: -96 | Explore2: 2.00\n",
      "Ep1: 17 | ---- | R1: -85 | Explore: 2.00\n",
      "Ep2: 17 | ---- | R2: -211 | Explore2: 2.00\n",
      "Ep1: 18 | ---- | R1: -206 | Explore: 2.00\n",
      "Ep2: 18 | ---- | R2: -97 | Explore2: 2.00\n",
      "Ep1: 19 | ---- | R1: -209 | Explore: 2.00\n",
      "Ep2: 19 | ---- | R2: -283 | Explore2: 2.00\n",
      "Ep1: 20 | ---- | R1: -221 | Explore: 2.00\n",
      "Ep2: 20 | ---- | R2: -139 | Explore2: 2.00\n",
      "Ep1: 21 | ---- | R1: -161 | Explore: 2.00\n",
      "Ep2: 21 | ---- | R2: -249 | Explore2: 2.00\n",
      "Ep1: 22 | ---- | R1: -182 | Explore: 2.00\n",
      "Ep2: 22 | ---- | R2: -244 | Explore2: 2.00\n",
      "Ep1: 23 | ---- | R1: -157 | Explore: 2.00\n",
      "Ep2: 23 | ---- | R2: -224 | Explore2: 2.00\n",
      "Ep1: 24 | ---- | R1: -95 | Explore: 2.00\n",
      "Ep2: 24 | ---- | R2: -211 | Explore2: 2.00\n",
      "Ep1: 25 | ---- | R1: -157 | Explore: 1.92\n",
      "Ep2: 25 | ---- | R2: -163 | Explore2: 1.92\n",
      "Ep1: 26 | ---- | R1: -37 | Explore: 1.85\n",
      "Ep2: 26 | ---- | R2: -174 | Explore2: 1.85\n",
      "Ep1: 27 | ---- | R1: -104 | Explore: 1.77\n",
      "Ep2: 27 | ---- | R2: -184 | Explore2: 1.77\n",
      "Ep1: 28 | ---- | R1: -112 | Explore: 1.70\n",
      "Ep2: 28 | ---- | R2: -140 | Explore2: 1.70\n",
      "Ep1: 29 | ---- | R1: -144 | Explore: 1.64\n",
      "Ep2: 29 | ---- | R2: -141 | Explore2: 1.64\n",
      "Ep1: 30 | ---- | R1: 1 | Explore: 1.57\n",
      "Ep2: 30 | ---- | R2: -83 | Explore2: 1.57\n",
      "Ep1: 31 | ---- | R1: -83 | Explore: 1.51\n",
      "Ep2: 31 | ---- | R2: -109 | Explore2: 1.51\n",
      "Ep1: 32 | ---- | R1: -106 | Explore: 1.45\n",
      "Ep2: 32 | ---- | R2: -20 | Explore2: 1.45\n",
      "Ep1: 33 | ---- | R1: -60 | Explore: 1.40\n",
      "Ep2: 33 | ---- | R2: -9 | Explore2: 1.40\n",
      "Ep1: 34 | ---- | R1: -128 | Explore: 1.34\n",
      "Ep2: 34 | ---- | R2: -45 | Explore2: 1.34\n",
      "Ep1: 35 | ---- | R1: -58 | Explore: 1.29\n",
      "Ep2: 35 | ---- | R2: -2 | Explore2: 1.29\n",
      "Ep1: 36 | ---- | R1: -51 | Explore: 1.24\n",
      "Ep2: 36 | ---- | R2: -109 | Explore2: 1.24\n",
      "Ep1: 37 | ---- | R1: -31 | Explore: 1.19\n",
      "Ep2: 37 | ---- | R2: 27 | Explore2: 1.19\n",
      "Ep1: 38 | ---- | R1: -41 | Explore: 1.14\n",
      "Ep2: 38 | ---- | R2: 35 | Explore2: 1.14\n",
      "Ep1: 39 | ---- | R1: -58 | Explore: 1.10\n",
      "Ep2: 39 | ---- | R2: 38 | Explore2: 1.10\n",
      "Ep1: 40 | ---- | R1: -157 | Explore: 1.05\n",
      "Ep2: 40 | ---- | R2: -239 | Explore2: 1.05\n",
      "Ep1: 41 | ---- | R1: -146 | Explore: 1.01\n",
      "Ep2: 41 | ---- | R2: -150 | Explore2: 1.01\n",
      "Ep1: 42 | ---- | R1: -203 | Explore: 0.97\n",
      "Ep2: 42 | ---- | R2: -273 | Explore2: 0.97\n",
      "Ep1: 43 | ---- | R1: -18 | Explore: 0.94\n",
      "Ep2: 43 | ---- | R2: 21 | Explore2: 0.94\n",
      "Ep1: 44 | ---- | R1: -11 | Explore: 0.90\n",
      "Ep2: 44 | ---- | R2: -40 | Explore2: 0.90\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil # copy(Mw)\n",
    "#from arm_env import ArmEnv\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(1)# seed ...sequence of numbers(mw)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "MAX_EPISODES = 600\n",
    "MAX_EP_STEPS = 200\n",
    "LR_A = 1e-4  # learning rate for actor\n",
    "LR_C = 1e-4  # learning rate for critic\n",
    "GAMMA = 0.9  # reward discount\n",
    "REPLACE_ITER_A = 1100\n",
    "REPLACE_ITER_C = 1000\n",
    "MEMORY_CAPACITY = 5000\n",
    "BATCH_SIZE = 16\n",
    "VAR_MIN = 0.1\n",
    "RENDER = True\n",
    "LOAD = False\n",
    "#LOAD =True\n",
    "MODE = ['easy', 'hard']\n",
    "n_model1 = 1\n",
    "n_model2 = 1\n",
    "\n",
    "env = ArmEnv(mode=MODE[n_model1])\n",
    "env = ArmEnv(mode=MODE[n_model2])\n",
    "\n",
    "STATE_DIM = env.state_dim\n",
    "ACTION_DIM = env.action_dim\n",
    "ACTION_BOUND = env.action_bound\n",
    "\n",
    "STATE_DIM2 = env.state_dim\n",
    "ACTION_DIM2 = env.action_dim\n",
    "ACTION_BOUND2 = env.action_bound\n",
    "\n",
    "# all placeholder for tf\n",
    "with tf.name_scope('S'):\n",
    "    S = tf.placeholder(tf.float32, shape=[None, STATE_DIM], name='s')\n",
    "with tf.name_scope('R1'):\n",
    "    R1 = tf.placeholder(tf.float32, [None, 1], name='r1')\n",
    "\n",
    "with tf.name_scope('R2'):\n",
    "    R2 = tf.placeholder(tf.float32, [None, 1], name='r2')\n",
    "\n",
    "with tf.name_scope('S_'):\n",
    "    S_ = tf.placeholder(tf.float32, shape=[None, STATE_DIM], name='s_')\n",
    "\n",
    "\n",
    "class Actor(object):\n",
    "    def __init__(self, sess, action_dim, action_bound, learning_rate, t_replace_iter):\n",
    "        self.sess = sess # A session allows to execute graphs\n",
    "        self.a_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.lr = learning_rate\n",
    "        self.t_replace_iter = t_replace_iter\n",
    "        self.t_replace_counter = 0\n",
    "        # It allows us to create our operations and build our computation graph, without needing the data.\n",
    "        with tf.variable_scope('Actor'):\n",
    "            # input s, output a\n",
    "            self.a = self._build_net(S, scope='eval_net', trainable=True)\n",
    "\n",
    "            # input s_, output a, get a_ for critic\n",
    "            self.a_ = self._build_net(S_, scope='target_net', trainable=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval_net')\n",
    "        self.t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target_net')\n",
    "\n",
    "\n",
    "\n",
    "    def _build_net(self, s, scope, trainable):\n",
    "        with tf.variable_scope(scope):\n",
    "            init_w = tf.contrib.layers.xavier_initializer()\n",
    "            init_b = tf.constant_initializer(0.001)\n",
    "            net = tf.layers.dense(s, 200, activation=tf.nn.relu6,\n",
    "                                  kernel_initializer=init_w, bias_initializer=init_b, name='l1',\n",
    "                                  trainable=trainable)\n",
    "            net = tf.layers.dense(net, 200, activation=tf.nn.relu6,\n",
    "                                  kernel_initializer=init_w, bias_initializer=init_b, name='l2',\n",
    "                                  trainable=trainable)\n",
    "            net = tf.layers.dense(net, 10, activation=tf.nn.relu,\n",
    "                                  kernel_initializer=init_w, bias_initializer=init_b, name='l3',\n",
    "                                  trainable=trainable)\n",
    "            with tf.variable_scope('a'):\n",
    "                actions = tf.layers.dense(net, self.a_dim, activation=tf.nn.tanh, kernel_initializer=init_w,\n",
    "                                          name='a', trainable=trainable)\n",
    "\n",
    "                scaled_a = tf.multiply(actions, self.action_bound, name='scaled_a')  # Scale output to -action_bound to action_bound\n",
    "        return scaled_a\n",
    "\n",
    "    def learn(self, s):   # batch update\n",
    "        self.sess.run(self.train_op, feed_dict={S: s})\n",
    "        if self.t_replace_counter % self.t_replace_iter == 0:\n",
    "            self.sess.run([tf.assign(t, e) for t, e in zip(self.t_params, self.e_params)])\n",
    "        self.t_replace_counter += 1\n",
    "\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = s[np.newaxis, :]  # single state\n",
    "        return self.sess.run(self.a, feed_dict={S: s})[0]  # single action\n",
    "\n",
    "    def add_grad_to_graph(self, a_grads):\n",
    "        with tf.variable_scope('policy_grads'):\n",
    "            self.policy_grads = tf.gradients(ys=self.a, xs=self.e_params, grad_ys=a_grads)\n",
    "\n",
    "        with tf.variable_scope('A_train'):\n",
    "            opt = tf.train.RMSPropOptimizer(-self.lr)  # (- learning rate) for ascent policy\n",
    "            self.train_op = opt.apply_gradients(zip(self.policy_grads, self.e_params))\n",
    "\n",
    "\n",
    "class Actor2(object):\n",
    "    def __init__(self, sess2, action_dim, action_bound, learning_rate, t_replace_iter):\n",
    "        self.sess2 = sess2 # A session allows to execute graphs(mw)\n",
    "        self.a_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.lr = learning_rate\n",
    "        self.t_replace_iter = t_replace_iter\n",
    "        self.t_replace_counter = 0\n",
    "        # It allows us to create our operations and build our computation graph, without needing the data.(mw)\n",
    "        with tf.variable_scope('Actor2'):\n",
    "            # input s, output a\n",
    "            self.a2 = self._build_net2(S, scope='eval_net2', trainable=True)\n",
    "\n",
    "            # input s_, output a, get a_ for critic\n",
    "            self.a2_ = self._build_net2(S_, scope='target_net2', trainable=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.e_params2 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor2/eval_net2')\n",
    "        self.t_params2 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor2/target_net2')\n",
    "\n",
    "\n",
    "\n",
    "    def _build_net2(self, s, scope, trainable):\n",
    "        with tf.variable_scope(scope):\n",
    "            init_w = tf.contrib.layers.xavier_initializer()\n",
    "            init_b = tf.constant_initializer(0.001)\n",
    "            net = tf.layers.dense(s, 200, activation=tf.nn.relu6,\n",
    "                                  kernel_initializer=init_w, bias_initializer=init_b, name='l1',\n",
    "                                  trainable=trainable)\n",
    "            net = tf.layers.dense(net, 200, activation=tf.nn.relu6,\n",
    "                                  kernel_initializer=init_w, bias_initializer=init_b, name='l2',\n",
    "                                  trainable=trainable)\n",
    "            net = tf.layers.dense(net, 10, activation=tf.nn.relu,\n",
    "                                  kernel_initializer=init_w, bias_initializer=init_b, name='l3',\n",
    "                                  trainable=trainable)\n",
    "            with tf.variable_scope('a2'):\n",
    "                actions2 = tf.layers.dense(net, self.a_dim, activation=tf.nn.tanh, kernel_initializer=init_w,\n",
    "                                          name='a2', trainable=trainable)\n",
    "\n",
    "                scaled_a2 = tf.multiply(actions2, self.action_bound, name='scaled_a2')  # Scale output to -action_bound to action_bound\n",
    "\n",
    "        return scaled_a2\n",
    "\n",
    "    def learn2(self, s):   # batch update\n",
    "        self.sess2.run(self.train_op2, feed_dict={S: s})\n",
    "        if self.t_replace_counter % self.t_replace_iter == 0:\n",
    "            self.sess2.run([tf.assign(t, e) for t, e in zip(self.t_params2, self.e_params2)])\n",
    "        self.t_replace_counter += 1\n",
    "\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = s[np.newaxis, :]    # single state\n",
    "        return self.sess2.run(self.a2, feed_dict={S: s})[0]  # single action\n",
    "\n",
    "    def add_grad_to_graph(self, a_grads):\n",
    "        with tf.variable_scope('policy_grads'):\n",
    "            self.policy_grads = tf.gradients(ys=self.a2, xs=self.e_params2, grad_ys=a_grads)\n",
    "\n",
    "        with tf.variable_scope('A_train'):\n",
    "            opt = tf.train.RMSPropOptimizer(-self.lr)  # (- learning rate) for ascent policy\n",
    "            self.train_op2 = opt.apply_gradients(zip(self.policy_grads, self.e_params2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Critic1(object):\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, gamma, t_replace_iter, a, a_):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.t_replace_iter = t_replace_iter\n",
    "        self.t_replace_counter = 0\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            # Input (s, a), output q\n",
    "            self.a = a\n",
    "            self.q = self._build_net(S, self.a, 'eval_net', trainable=True)\n",
    "\n",
    "            # Input (s_, a_), output q_ for q_target\n",
    "            self.q_ = self._build_net(S_, a_, 'target_net', trainable=False)    # target_q is based on a_ from Actor's target_net\n",
    "\n",
    "            self.e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval_net')\n",
    "            self.t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target_net')\n",
    "\n",
    "        with tf.variable_scope('target_q'):\n",
    "            self.target_q = R1 + self.gamma * self.q_\n",
    "\n",
    "        with tf.variable_scope('TD_error'):\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(self.target_q, self.q))\n",
    "\n",
    "        with tf.variable_scope('C_train'):\n",
    "            self.train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)\n",
    "\n",
    "        with tf.variable_scope('a_grad'):\n",
    "            self.a_grads = tf.gradients(self.q, a)[0]   # tensor of gradients of each sample (None, a_dim)\n",
    "\n",
    "    def _build_net(self, s, a, scope, trainable):\n",
    "        with tf.variable_scope(scope):\n",
    "            init_w = tf.contrib.layers.xavier_initializer()\n",
    "            init_b = tf.constant_initializer(0.01)\n",
    "\n",
    "            with tf.variable_scope('l1'):\n",
    "                n_l1 = 200\n",
    "                w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], initializer=init_w, trainable=trainable)\n",
    "                w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], initializer=init_w, trainable=trainable)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=init_b, trainable=trainable)\n",
    "                net = tf.nn.relu6(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)\n",
    "            net = tf.layers.dense(net, 200, activation=tf.nn.relu6,\n",
    "                                  kernel_initializer=init_w, bias_initializer=init_b, name='l2',\n",
    "                                  trainable=trainable)\n",
    "            net = tf.layers.dense(net, 10, activation=tf.nn.relu,\n",
    "                                  kernel_initializer=init_w, bias_initializer=init_b, name='l3',\n",
    "                                  trainable=trainable)\n",
    "            with tf.variable_scope('q'):\n",
    "                q = tf.layers.dense(net, 1, kernel_initializer=init_w, bias_initializer=init_b, trainable=trainable)   # Q(s,a)\n",
    "        return q\n",
    "\n",
    "\n",
    "\n",
    "    def learn(self, s, a, r1, s_):\n",
    "        self.sess.run(self.train_op, feed_dict={S: s, self.a: a, R1: r1, S_: s_})\n",
    "        if self.t_replace_counter % self.t_replace_iter == 0:\n",
    "            self.sess.run([tf.assign(t, e) for t, e in zip(self.t_params, self.e_params)])\n",
    "        self.t_replace_counter += 1\n",
    "\n",
    "\n",
    "class Critic2(object):\n",
    "    def __init__(self, sess2, state_dim, action_dim, learning_rate, gamma, t_replace_iter, a2, a2_):\n",
    "        self.sess2 = sess2\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.t_replace_iter = t_replace_iter\n",
    "        self.t_replace_counter = 0\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            # Input (s, a), output q\n",
    "            self.a2 = a2\n",
    "            self.q2 = self._build_net2(S, self.a2, 'eval_net2', trainable=True)\n",
    "\n",
    "            # Input (s_, a_), output q_ for q_target\n",
    "            self.q2_ = self._build_net2(S_, a2_, 'target_net2', trainable=False)    # target_q is based on a_ from Actor's target_net\n",
    "\n",
    "            self.e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval_net2')\n",
    "            self.t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target_net2')\n",
    "\n",
    "        with tf.variable_scope('target_q'):\n",
    "            self.target_q2 = R2 + self.gamma * self.q2_\n",
    "\n",
    "        with tf.variable_scope('TD_error'):\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(self.target_q2, self.q2))\n",
    "\n",
    "        with tf.variable_scope('C_train'):\n",
    "            self.train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)\n",
    "\n",
    "        with tf.variable_scope('a_grad'):\n",
    "            self.a_grads = tf.gradients(self.q2, a2)[0]   # tensor of gradients of each sample (None, a_dim)\n",
    "\n",
    "    def _build_net2(self, s, a2, scope, trainable):\n",
    "        with tf.variable_scope(scope):\n",
    "            init_w = tf.contrib.layers.xavier_initializer()\n",
    "            init_b = tf.constant_initializer(0.01)\n",
    "\n",
    "            with tf.variable_scope('l1'):\n",
    "                n_l1 = 200\n",
    "                w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], initializer=init_w, trainable=trainable)\n",
    "                w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], initializer=init_w, trainable=trainable)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=init_b, trainable=trainable)\n",
    "                net = tf.nn.relu6(tf.matmul(s, w1_s) + tf.matmul(a2, w1_a) + b1)\n",
    "            net = tf.layers.dense(net, 200, activation=tf.nn.relu6,\n",
    "                                  kernel_initializer=init_w, bias_initializer=init_b, name='l2',\n",
    "                                  trainable=trainable)\n",
    "            net = tf.layers.dense(net, 10, activation=tf.nn.relu,\n",
    "                                  kernel_initializer=init_w, bias_initializer=init_b, name='l3',\n",
    "                                  trainable=trainable)\n",
    "            with tf.variable_scope('q'):\n",
    "                q2 = tf.layers.dense(net, 1, kernel_initializer=init_w, bias_initializer=init_b, trainable=trainable)   # Q(s,a)\n",
    "\n",
    "        return q2\n",
    "\n",
    "\n",
    "    def learn2(self, s, a2, r2, s_):\n",
    "        self.sess2.run(self.train_op, feed_dict={S: s, self.a2: a2, R2: r2, S_: s_})\n",
    "        if self.t_replace_counter % self.t_replace_iter == 0:\n",
    "            self.sess2.run([tf.assign(t, e) for t, e in zip(self.t_params, self.e_params)])\n",
    "        self.t_replace_counter += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Memory1(object):\n",
    "    def __init__(self, capacity, dims):\n",
    "        self.capacity = capacity\n",
    "        self.data = np.zeros((capacity, dims))\n",
    "        self.pointer = 0\n",
    "\n",
    "    def store_transition(self, s, a, r1, s_):\n",
    "        transition = np.hstack((s, a, [r1], s_))\n",
    "        index = self.pointer % self.capacity  # replace the old memory with new memory\n",
    "        self.data[index, :] = transition\n",
    "        self.pointer += 1\n",
    "\n",
    "    def sample(self, n):\n",
    "        assert self.pointer >= self.capacity, 'Memory has not been fulfilled'\n",
    "        indices = np.random.choice(self.capacity, size=n)\n",
    "        return self.data[indices, :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Memory2(object):\n",
    "    def __init__(self, capacity, dims):\n",
    "        self.capacity = capacity\n",
    "        self.data = np.zeros((capacity, dims))\n",
    "        self.pointer = 0\n",
    "\n",
    "    def store_transition2(self, s, a2, r2, s_):\n",
    "        transition = np.hstack((s, a2, [r2], s_))\n",
    "        index = self.pointer % self.capacity  # replace the old memory with new memory\n",
    "        self.data[index, :] = transition\n",
    "        self.pointer += 1\n",
    "\n",
    "    def sample2(self, n):\n",
    "        assert self.pointer >= self.capacity, 'Memory2 has not been fulfilled'\n",
    "        indices = np.random.choice(self.capacity, size=n)\n",
    "        return self.data[indices, :]\n",
    "\n",
    "\n",
    "sess2 = tf.Session()\n",
    "\n",
    "# Create actor and critic.\n",
    "actor = Actor(sess2, ACTION_DIM, ACTION_BOUND[1], LR_A, REPLACE_ITER_A)\n",
    "critic = Critic1(sess2, STATE_DIM, ACTION_DIM, LR_C, GAMMA, REPLACE_ITER_C, actor.a, actor.a_)\n",
    "actor.add_grad_to_graph(critic.a_grads)\n",
    "\n",
    "M1 = Memory1(MEMORY_CAPACITY, dims=2 * STATE_DIM + ACTION_DIM + 1)\n",
    "\n",
    "\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "path = './'+MODE[1]\n",
    "\n",
    "if LOAD:\n",
    "    saver.restore(sess2, tf.train.latest_checkpoint(path))\n",
    "else:\n",
    "    sess2.run(tf.global_variables_initializer())\n",
    "\n",
    "# Create actor and critic.\n",
    "actor2 = Actor2(sess2, ACTION_DIM2, ACTION_BOUND2[1], LR_A, REPLACE_ITER_A)\n",
    "critic2 = Critic2(sess2, STATE_DIM2, ACTION_DIM2, LR_C, GAMMA, REPLACE_ITER_C, actor2.a2, actor2.a2_)\n",
    "actor2.add_grad_to_graph(critic2.a_grads)\n",
    "\n",
    "M2 = Memory2(MEMORY_CAPACITY, dims=2 * STATE_DIM + ACTION_DIM + 1)\n",
    "\n",
    "\n",
    "\n",
    "saver2 = tf.train.Saver(max_to_keep=100)\n",
    "path2 = './'+MODE[1]\n",
    "\n",
    "if LOAD:\n",
    "    saver2.restore(sess2, tf.train.latest_checkpoint(path2))\n",
    "else:\n",
    "    sess2.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train():\n",
    "    var = 2.  # control exploration\n",
    "\n",
    "    for ep1 in range(MAX_EPISODES):\n",
    "        ep2 = ep1\n",
    "        s1 = env.reset1()\n",
    "        s2 = env.reset2()\n",
    "        ep_reward1 = 0\n",
    "        ep_reward2 = 0\n",
    "\n",
    "\n",
    "        for t in range(MAX_EP_STEPS):\n",
    "            # while True:\n",
    "            if RENDER:\n",
    "                env.render()\n",
    "\n",
    "\n",
    "\n",
    "            # Added exploration noise\n",
    "            a = actor.choose_action(s1)\n",
    "            a2 = actor2.choose_action(s2)\n",
    "            a = np.clip(np.random.normal(a, var), *ACTION_BOUND)\n",
    "            a2 = np.clip(np.random.normal(a2, var), *ACTION_BOUND2)  # add randomness to action selection for exploration\n",
    "            s1_, r1, done = env.step1(a)\n",
    "            s2_, r2, done = env.step2(a2)\n",
    "            M1.store_transition(s1, a, r1, s1_)\n",
    "            M2.store_transition2(s2, a2, r2, s2_)\n",
    "\n",
    "            if M1.pointer > MEMORY_CAPACITY:\n",
    "                var = max([var * .9999, VAR_MIN])  # decay the action randomness\n",
    "                b_M = M1.sample(BATCH_SIZE)\n",
    "                b_s = b_M[:, :STATE_DIM2]\n",
    "                b_a = b_M[:, STATE_DIM2: STATE_DIM2 + ACTION_DIM2]\n",
    "                b_r1 = b_M[:, -STATE_DIM - 1: -STATE_DIM]\n",
    "                b_s_ = b_M[:, -STATE_DIM2:]\n",
    "\n",
    "                critic.learn(b_s, b_a, b_r1, b_s_)\n",
    "                actor.learn(b_s)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if M2.pointer > MEMORY_CAPACITY:\n",
    "                var = max([var * .9999, VAR_MIN])  # decay the action randomness\n",
    "                b_M = M2.sample2(BATCH_SIZE)\n",
    "                b_s = b_M[:, :STATE_DIM2]\n",
    "                b_a = b_M[:, STATE_DIM2: STATE_DIM2 + ACTION_DIM2]\n",
    "                b_r2 = b_M[:, -STATE_DIM2 - 1: -STATE_DIM2]\n",
    "                b_s_ = b_M[:, -STATE_DIM2:]\n",
    "\n",
    "                critic2.learn2(b_s, b_a, b_r2, b_s_)\n",
    "                actor2.learn2(b_s)\n",
    "\n",
    "            s1 = s1_\n",
    "            s2 = s2_\n",
    "            ep_reward1 += r1\n",
    "            ep_reward2 += r2\n",
    "\n",
    "            if t == MAX_EP_STEPS - 1 or done:\n",
    "                # if done:\n",
    "                result = '| done' if done else '| ----'\n",
    "                print('Ep1:', ep1,\n",
    "                      result,\n",
    "                      '| R1: %i' % int(ep_reward1),\n",
    "                      '| Explore: %.2f' % var,\n",
    "                      )\n",
    "\n",
    "\n",
    "            if t == MAX_EP_STEPS - 1 or done:\n",
    "                # if done:\n",
    "                result = '| done' if done else '| ----'\n",
    "                print('Ep2:', ep2,\n",
    "                      result,\n",
    "                      '| R2: %i' % int(ep_reward2),\n",
    "                      '| Explore2: %.2f' % var,\n",
    "                      )\n",
    "                break\n",
    "\n",
    "    if os.path.isdir(path): shutil.rmtree(path)\n",
    "    os.mkdir(path)\n",
    "    if os.path.isdir(path2): shutil.rmtree(path2)\n",
    "    os.mkdir(path2)\n",
    "    ckpt_path = os.path.join('./' + MODE[n_model1], 'Actor Critic.ckpt1')\n",
    "    ckpt_path2 = os.path.join('./' + MODE[n_model2], 'Actor Critic.ckpt2')\n",
    "    save_path = saver.save(sess2, ckpt_path, write_meta_graph=False)\n",
    "    save_path2 = saver2.save(sess2, ckpt_path2, write_meta_graph=False)\n",
    "    print(\"\\nSave Model1 %s\\n\" % save_path)\n",
    "    print(\"\\nSave Model2 %s\\n\" % save_path2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def eval():\n",
    "    for ep1 in range(MAX_EPISODES):\n",
    "        env.set_fps(100)\n",
    "\n",
    "        s1 = env.reset1()\n",
    "        s2 = env.reset2()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for t in range(MAX_EP_STEPS):\n",
    "\n",
    "               if RENDER:\n",
    "                  env.render()\n",
    "               a1 = actor.choose_action(s1)\n",
    "               s1_, r1, done = env.step1(a1)\n",
    "               s1 = s1_\n",
    "\n",
    "               a2 = actor2.choose_action(s2)\n",
    "               s2_, r2, done = env.step2(a2)\n",
    "               s2 = s2_\n",
    "               print(\"Program is still running......\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if LOAD:\n",
    "        eval()\n",
    "    else:\n",
    "\n",
    "        train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
